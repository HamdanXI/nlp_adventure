{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6NI089SSmpEF",
        "gFzNdxZKnbrJ",
        "HsPrUIrAn4At",
        "wxvJ-G5xouQ1",
        "mLtpcO3apa47",
        "CcfWfJ5grOrT"
      ],
      "authorship_tag": "ABX9TyOe1BQy5/GmVr+ZqPMwQdS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/exam/prepare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qualifying Exam Preperation"
      ],
      "metadata": {
        "id": "qJTmB0ZfnmJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traditional NLP"
      ],
      "metadata": {
        "id": "DV6UqKmimil2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "6NI089SSmpEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is Stemming?\n",
        "\n",
        "Stemming is the process of reducing words to their word stem, base, or root formâ€”generally a written word form. The idea is to remove affixes (prefixes and suffixes) from words in order to obtain a form that is often not a complete word by itself but is representative of related words. For instance, \"running\", \"runner\", and \"ran\" all stem from the root \"run\".\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is Stemming Used?\n",
        "\n",
        "1. **Simplification**: It simplifies textual data, reducing the complexity of subsequent NLP tasks.\n",
        "2. **Speed**: Stemming is generally faster than more complex methods like lemmatization because it uses simple heuristics.\n",
        "3. **Efficiency**: It increases the efficiency of information retrieval by linking words with the same roots.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How Does Stemming Work?\n",
        "\n",
        "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This process is fairly crude; the stems may not be actual words. For example:\n",
        "- **Porter Stemmer**: One of the most common and gentle stemmers. It's known for its simplicity and speed.\n",
        "- **Lancaster Stemmer**: A more aggressive stemmer than the Porter, often resulting in shorter stems, hence more errors if accuracy is critical.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges with Stemming\n",
        "\n",
        "- **Over-stemming**: Occurs when two words are stemmed to the same root that are not of the same root. This can lead to a loss of information important for understanding the original word.\n",
        "- **Under-stemming**: Occurs when two words that should be stemmed to the same root are not. This can lead to inconsistent results in search queries and information retrieval.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Example in Python\n",
        "\n",
        "Using the NLTK library, let's look at how to use both the Porter and Lancaster stemmers:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "words = [\"running\", \"eats\", \"flying\", \"quickly\"]\n",
        "porter_stems = [porter.stem(word) for word in words]\n",
        "lancaster_stems = [lancaster.stem(word) for word in words]\n",
        "\n",
        "print(\"Porter Stems:\", porter_stems)\n",
        "print(\"Lancaster Stems:\", lancaster_stems)\n",
        "```\n",
        "\n",
        "This script will output the stems of the words using both stemmers:\n",
        "\n",
        "```plaintext\n",
        "Porter Stems: ['run', 'eat', 'fli', 'quickli']\n",
        "Lancaster Stems: ['run', 'eat', 'fly', 'quick']\n",
        "```\n",
        "\n",
        "As you can see, the Lancaster stemmer generally produces more aggressive cuts. These stemming techniques are useful in scenarios where the exact form of a word is less important than linking variants of the word to the same base form."
      ],
      "metadata": {
        "id": "RT92ZNGlnxgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "gFzNdxZKnbrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is Lemmatization?\n",
        "\n",
        "Lemmatization is the process of reducing a word to its base or root form, known as the lemma. Unlike stemming, which crudely chops off word endings to achieve the root form, lemmatization considers the morphological analysis of the word, ensuring that the reduced form is a valid word according to the language's vocabulary. This process involves understanding the context and part of speech of a word in a sentence, as well as its standard form according to a language's rules.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is Lemmatization Important?\n",
        "\n",
        "1. **Reduces Complexity**: It decreases the complexity of text data by reducing variations of the same word to a single form, which improves the performance of various NLP tasks.\n",
        "2. **Improves Accuracy**: Since lemmatization keeps the semantic meaning of the word intact, it's more accurate than stemming for tasks that need a higher level of understanding, such as semantic analysis.\n",
        "3. **Facilitates Better Text Analysis**: By converting words to their base forms, it becomes easier to perform tasks like textual comparison and pattern recognition.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How Does Lemmatization Work?\n",
        "\n",
        "Lemmatization works by using vocabulary and morphological analysis of words. The goal is to remove only inflectional endings and return the base or dictionary form of a word, which is known as the lemma. For instance, the lemma of \"was\" is \"be,\" and the lemma of \"mice\" is \"mouse.\"\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges in Lemmatization\n",
        "\n",
        "- **Language Dependency**: Lemmatization rules can be complex and highly language-dependent. For example, in English, verbs and nouns are lemmatized differently.\n",
        "- **Resource Intensive**: It requires more computational resources than stemming, as it needs a complete dictionary of lemmas and morphological analysis.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Example in Python\n",
        "\n",
        "Using the popular NLP library called NLTK, here is how you can perform lemmatization:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('wordnet')  # Download the necessary lexicons\n",
        "nltk.download('omw-1.4')  # Download the additional WordNet data\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"ran\", \"run\", \"easily\", \"fairer\"]\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmas)\n",
        "```\n",
        "\n",
        "This code snippet will output the lemmas of the provided words:\n",
        "\n",
        "```plaintext\n",
        "['running', 'ran', 'run', 'easily', 'fairer']\n",
        "```\n",
        "\n",
        "It's important to note that without specifying the part of speech (POS), the lemmatizer treats every word as a noun, which can lead to incorrect lemmas for verbs and adjectives."
      ],
      "metadata": {
        "id": "HsPrUIrAn4At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model for English\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to lemmatize a sentence\n",
        "def lemmatize_sentence(sentence):\n",
        "    # Process the sentence using spaCy\n",
        "    doc = nlp(sentence)\n",
        "    # Extract the lemma for each token and return them\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return lemmas\n",
        "\n",
        "# Example usage\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "lemmatized_tokens = lemmatize_sentence(sentence)\n",
        "print(\"Lemmatized tokens:\", lemmatized_tokens)"
      ],
      "metadata": {
        "id": "5Mt0OmIzoeRH",
        "outputId": "20db015e-16f9-416a-edc6-3904b33d3082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens: ['the', 'stripe', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "wxvJ-G5xouQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is Tokenization?\n",
        "\n",
        "Tokenization is a fundamental step in natural language processing (NLP) where text is divided into smaller units called tokens. These tokens can be words, numbers, or punctuation marks. The process helps in preparing text for deeper processing like parsing, part of speech tagging, and sentiment analysis.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is Tokenization Important?\n",
        "\n",
        "1. **Simplification**: It simplifies text analysis by breaking down large pieces of text into manageable units.\n",
        "2. **Standardization**: Tokens become the standard input for most NLP tasks.\n",
        "3. **Efficiency**: It increases the efficiency of other NLP processes, as they can operate on simplified and standardized data.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Types of Tokenization\n",
        "\n",
        "1. **Word Tokenization**: Splits text into words. It's the most common form, useful for tasks like frequency analysis and word-level processing.\n",
        "2. **Sentence Tokenization**: Breaks text into sentences. This is useful for tasks that require understanding the context or meaning of sentences, like summarization.\n",
        "3. **Subword Tokenization**: Divides words into smaller meaningful units (subwords or morphemes). This is particularly useful in language modeling and machine translation to handle rare words or morphologically rich languages.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges\n",
        "\n",
        "- **Complexity in Different Languages**: Languages with no clear word boundaries (like Chinese or Japanese) require more sophisticated techniques beyond whitespace-based tokenization.\n",
        "- **Handling Special Cases**: Punctuation, contractions (like \"don't\"), and special characters can complicate straightforward splits.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Examples in Python\n",
        "\n",
        "Using a popular NLP library in Python called NLTK, here's how you can perform word tokenization:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')  # Download the necessary models\n",
        "\n",
        "text = \"Hello, how are you doing today?\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "This would output:\n",
        "\n",
        "```plaintext\n",
        "['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n",
        "```"
      ],
      "metadata": {
        "id": "4MpJGtw1o0pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking"
      ],
      "metadata": {
        "id": "mLtpcO3apa47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is Chunking?\n",
        "Chunking, also known as shallow parsing, is the process of extracting phrases from unstructured text and grouping together consecutive words into larger unitsâ€”commonly known as \"chunks.\" Instead of just identifying parts of speech, chunking groups words into meaningful sequences like noun phrases or verb phrases that provide more structure than individual words for processing.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is Chunking Important?\n",
        "1. **Structure Extraction**: Chunking helps extract more structure from text than individual tokenization or POS tagging by identifying the constituents of sentences.\n",
        "2. **Information Retrieval**: It aids in extracting entities (like names or places) and relations between them, which is crucial for tasks like information extraction and named entity recognition.\n",
        "3. **Improves Understanding**: By identifying phrases, chunking helps in understanding the context and the syntactic meaning of the text better.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How Does Chunking Work?\n",
        "Chunking usually works on top of part-of-speech (POS) tagging and uses rules or machine learning models to identify the different chunks. For example, a simple rule might be to group any combination of an optional determiner followed by any number of adjectives and then a noun into a noun phrase (NP).\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Rules for Chunking\n",
        "A common way to perform chunking is to use regular-expression-based rules. For example:\n",
        "- **NP**: `{<DT>?<JJ>*<NN>}` - This rule states that a noun phrase, NP, might start with an optional determiner (DT), followed by any number of adjectives (JJ), and ends with a noun (NN).\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Example in Python\n",
        "Using NLTK, we can implement chunking as follows:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Define your chunk pattern\n",
        "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "\n",
        "# Create a chunk parser\n",
        "cp = RegexpParser(pattern)\n",
        "cs = cp.parse(tagged_tokens)\n",
        "\n",
        "# Display the chunked sentence\n",
        "print(cs)\n",
        "cs.draw()\n",
        "```\n",
        "\n",
        "In this example, the script tokenizes the sentence, tags each token with its part of speech, and then applies a chunking pattern to identify noun phrases. The `cs.draw()` method would display the sentence structure graphically, showing which parts of the sentence are grouped as noun phrases.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges in Chunking\n",
        "\n",
        "- **Complex Patterns**: Designing rules that accurately capture the intended chunks without being too general or too specific can be challenging.\n",
        "- **Language Dependence**: Chunking rules can be highly dependent on the language and may not transfer well from one language to another without adjustments."
      ],
      "metadata": {
        "id": "ogFrqF3tpcHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PoS Tagging"
      ],
      "metadata": {
        "id": "9cxtjc4YTtHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is PoS Tagging?\n",
        "Part-of-speech tagging, or PoS tagging, involves assigning a part-of-speech (such as noun, verb, adjective, etc.) to each word in a given text, based on both its definition and its context within a sentence. This is fundamental for syntactic parsing and text analysis.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is PoS Tagging Important?\n",
        "1. **Syntax Analysis**: It helps in understanding the grammatical structure of sentences, which is crucial for higher-level NLP tasks like parsing and entity recognition.\n",
        "2. **Disambiguation**: Helps in resolving ambiguities in language by clarifying whether a word is used as a noun, verb, or adjective in a particular context.\n",
        "3. **Improves Machine Translation**: Accurate tagging is crucial for effective translation, as it helps in constructing sentences correctly in the target language.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How Does PoS Tagging Work?\n",
        "\n",
        "PoS tagging can be performed using different methods:\n",
        "- **Rule-Based Tagging**: Uses hand-written rules to decide the tag based on the wordsâ€™ affixes and the context in which they appear.\n",
        "- **Stochastic Tagging**: Relies on statistical models, often trained on a tagged corpus, to predict the most likely tag based on the word itself and its surrounding context.\n",
        "- **Machine Learning Approaches**: Modern taggers use more complex models, including Hidden Markov Models (HMM), Conditional Random Fields (CRF), and neural networks, which can learn from large amounts of data and capture more subtle distinctions in how words are used.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Example in Python\n",
        "\n",
        "Using the NLTK library, which provides access to some pre-trained PoS taggers, here's how you can tag a sentence:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "print(tagged_tokens)\n",
        "```\n",
        "\n",
        "This script will output a list of tuples, where each tuple consists of a word and its corresponding part-of-speech tag based on the default English PoS tagger:\n",
        "\n",
        "```plaintext\n",
        "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges in PoS Tagging\n",
        "\n",
        "- **Ambiguity**: Some words can represent more than one part of speech based on the context, making PoS tagging non-trivial.\n",
        "- **Domain-Specific Language**: Specialized vocabularies, such as medical or legal terminologies, can have different usage patterns that standard taggers may not handle well.\n",
        "- **New Words and Slangs**: Languages evolve, and new words, slangs, and terminologies may not be recognized by existing models."
      ],
      "metadata": {
        "id": "dSDHtguHTvFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing"
      ],
      "metadata": {
        "id": "NRPvBkH2VhtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is Parsing?\n",
        "\n",
        "Parsing, in the context of NLP, refers to the process of analyzing a text, conforming to the rules of formal grammar, to identify its grammatical structure with respect to a given language. This involves breaking down a text into its component parts and then understanding how these parts fit together in a structured form, often represented as a parse tree.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Types of Parsing\n",
        "\n",
        "1. **Constituency Parsing**: This type focuses on the structure of the sentence as defined by the grammar of the language. It breaks a text into sub-phrases, known as constituents, which can be nested within each other. The result is often visualized as a tree, where each node represents a constituent.\n",
        "   \n",
        "2. **Dependency Parsing**: Rather than focusing on sub-phrases, dependency parsing builds relationships based on word dependencies. Each sentence is represented as a directed graph, where nodes are words and edges are dependencies between the words, such as subject or object relationships.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is Parsing Important?\n",
        "\n",
        "- **Syntax Analysis**: Parsing is crucial for understanding the syntactic structure of sentences, which is fundamental for numerous applications like machine translation, question answering, and speech recognition.\n",
        "- **Information Extraction**: It aids in extracting structured information from text, such as in legal documents or technical manuals.\n",
        "- **Improving Language Understanding**: Advanced parsing helps machines understand complex linguistic constructs, enhancing natural language understanding systems.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How Does Parsing Work?\n",
        "\n",
        "Parsing algorithms can be broadly classified into two categories:\n",
        "- **Rule-Based Parsing**: Uses a set of predefined grammar rules and attempts to apply these rules to find out the sentence structure. Common algorithms include top-down parsing, bottom-up parsing, and chart parsing.\n",
        "- **Statistical Parsing**: Employs statistical methods and machine learning models trained on corpora containing sentences and their correct structures. These parsers predict the most likely structure of a new sentence based on learned patterns.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Example in Python\n",
        "\n",
        "Using the popular NLTK library, hereâ€™s how you can implement a simple parser:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk import CFG\n",
        "from nltk.parse import ChartParser\n",
        "\n",
        "# Define a context-free grammar for a small subset of English\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    VP -> V NP\n",
        "    NP -> D N\n",
        "    V -> \"eats\" | \"drinks\"\n",
        "    D -> \"the\" | \"a\"\n",
        "    N -> \"man\" | \"fish\" | \"apple\" | \"water\"\n",
        "\"\"\")\n",
        "\n",
        "# Create a parser\n",
        "parser = ChartParser(grammar)\n",
        "\n",
        "# Parse a sentence\n",
        "sentence = 'the man eats a fish'.split()\n",
        "trees = list(parser.parse(sentence))\n",
        "\n",
        "for tree in trees:\n",
        "    print(tree)\n",
        "    tree.draw()\n",
        "```\n",
        "\n",
        "This script will output a parse tree showing the structure of the sentence according to the defined grammar. The `tree.draw()` method visualizes this structure.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges in Parsing\n",
        "\n",
        "- **Complexity**: Parsing is computationally expensive, especially for longer sentences with more complex structures.\n",
        "- **Ambiguity**: Natural language is often ambiguous, making it difficult to identify a single correct parse tree.\n",
        "- **Coverage**: Rule-based parsers require comprehensive and detailed grammar rules, which are hard to define for all possible sentences in a language."
      ],
      "metadata": {
        "id": "vviUSqIcVjZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linguistic Ambiguity"
      ],
      "metadata": {
        "id": "N7WjtQTkVwNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is Linguistic Ambiguity?\n",
        "It refers to the phenomenon where a sentence, phrase, word, or even a sound can be interpreted in multiple ways. Understanding and resolving ambiguity is a key challenge in natural language processing (NLP).\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Types of Linguistic Ambiguity\n",
        "\n",
        "1. **Lexical Ambiguity**: This occurs when a word has multiple meanings. For example, the word \"bank\" can refer to the side of a river or a financial institution.\n",
        "\n",
        "2. **Syntactic Ambiguity**: Also known as structural ambiguity, this happens when a sentence can be parsed in more than one way. For example, \"I saw the man with a telescope\" can mean either that I used a telescope to see the man or that the man I saw had a telescope.\n",
        "\n",
        "3. **Semantic Ambiguity**: This type of ambiguity arises when the meanings of sentences are unclear or have multiple interpretations beyond individual word meanings or syntactic structure. For instance, \"Heâ€™s visiting the bank\" doesnâ€™t specify which type of bank.\n",
        "\n",
        "4. **Pragmatic Ambiguity**: Occurs when the context does not provide enough information to clarify the meaning, even though the words and structure are clear. For example, the response \"I have\" in answer to the question \"Do you have the time or the inclination?\" is pragmatically ambiguous.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is Linguistic Ambiguity Important?\n",
        "\n",
        "- **Language Understanding**: Ambiguity is inherent in language and understanding it is crucial for effective communication, humor, and language richness.\n",
        "- **NLP Applications**: Ambiguity presents both a challenge and an opportunity in NLP tasks like machine translation, speech recognition, and sentiment analysis, where different interpretations can lead to different outcomes.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Resolving Ambiguity\n",
        "\n",
        "Resolving ambiguity is often context-dependent and can involve several strategies:\n",
        "\n",
        "- **Syntactic Parsing**: Techniques like parsing can help determine the most likely structure of a sentence and clarify syntactic ambiguity.\n",
        "- **Semantic Analysis**: Tools like word sense disambiguation are used to determine which meaning of a word is being used in a context.\n",
        "- **Pragmatic Understanding**: Understanding the speakerâ€™s intent and the conversational context can help resolve many ambiguities, particularly pragmatic ambiguity.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges\n",
        "\n",
        "- **High Complexity**: Resolving ambiguity often requires understanding the full context in which communication occurs, which can be highly complex.\n",
        "- **Computational Difficulty**: Ambiguity resolution is computationally expensive and can be challenging for algorithms to handle accurately, especially in real-time applications.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Example in NLP\n",
        "\n",
        "Handling ambiguity in NLP typically involves a combination of linguistic data and statistical models. For instance, modern NLP systems like BERT (Bidirectional Encoder Representations from Transformers) use context heavily to determine word meanings, which helps in disambiguating sentences more effectively."
      ],
      "metadata": {
        "id": "AiILWv2CVyjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All-in-One Code"
      ],
      "metadata": {
        "id": "CcfWfJ5grOrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Libraries\n",
        "%%capture\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer"
      ],
      "metadata": {
        "id": "Ds5aPjaHscXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentence):\n",
        "  # Tokenization\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "  print('Tokenization: ', tokens)\n",
        "\n",
        "  # Lemmatizer\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "  print('Lemmatization: ', lemmas)\n",
        "\n",
        "  # Stemming\n",
        "  porter = PorterStemmer()\n",
        "  lancaster = LancasterStemmer()\n",
        "\n",
        "  porter_stems = [porter.stem(token) for token in tokens]\n",
        "  lancaster_stems = [lancaster.stem(token) for token in tokens]\n",
        "\n",
        "  print(\"Stemming (Porter Stems): \", porter_stems)\n",
        "  print(\"Stemming (Lancaster Stems): \", lancaster_stems)\n",
        "\n",
        "\n",
        "  # Chunking\n",
        "  tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "  # Define your chunk pattern - A common way to perform chunking is to use regular-expression-based rules. For example:\n",
        "  pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "  # This rule states that a noun phrase, NP, might start with an optional determiner (DT), followed by any number of adjectives (JJ),\n",
        "  # and ends with a noun (NN).\n",
        "\n",
        "  # Create a chunk parser\n",
        "  cp = RegexpParser(pattern)\n",
        "  cs = cp.parse(tagged_tokens)\n",
        "  print(\"Chunking: \", cs)"
      ],
      "metadata": {
        "id": "y7CTrORErN_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"The quick brown fox jumps over the lazy dog\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2lBnvjEsFLG",
        "outputId": "5b6bbb3e-299f-48e5-caf9-e3f2f04ac68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "Lemmatization:  ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog']\n",
            "Stemming (Porter Stems):  ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n",
            "Stemming (Lancaster Stems):  ['the', 'quick', 'brown', 'fox', 'jump', 'ov', 'the', 'lazy', 'dog']\n",
            "Chunking:  (S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"We ran the running marathon together.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBL5s9lwts90",
        "outputId": "0a7e9e2b-d036-4eb8-e3d0-fdbbbfa1e5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:  ['We', 'ran', 'the', 'running', 'marathon', 'together', '.']\n",
            "Lemmatization:  ['We', 'ran', 'the', 'running', 'marathon', 'together', '.']\n",
            "Stemming (Porter Stems):  ['we', 'ran', 'the', 'run', 'marathon', 'togeth', '.']\n",
            "Stemming (Lancaster Stems):  ['we', 'ran', 'the', 'run', 'marathon', 'togeth', '.']\n",
            "Chunking:  (S\n",
            "  We/PRP\n",
            "  ran/VBD\n",
            "  the/DT\n",
            "  running/VBG\n",
            "  (NP marathon/NN)\n",
            "  together/RB\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nfkZY5UJtzxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}