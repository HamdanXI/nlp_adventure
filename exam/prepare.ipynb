{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6NI089SSmpEF",
        "gFzNdxZKnbrJ",
        "HsPrUIrAn4At",
        "wxvJ-G5xouQ1",
        "mLtpcO3apa47",
        "CcfWfJ5grOrT"
      ],
      "authorship_tag": "ABX9TyP/nxW6GGSM3y+xID5AqP1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/exam/prepare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qualifying Exam Preperation"
      ],
      "metadata": {
        "id": "qJTmB0ZfnmJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traditional NLP"
      ],
      "metadata": {
        "id": "DV6UqKmimil2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "6NI089SSmpEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is it?\n",
        "Stemming is the process of reducing words to their word stem, base, or root formâ€”generally a written word form. The idea is to remove affixes (prefixes and suffixes) from words in order to obtain a form that is often not a complete word by itself but is representative of related words. For instance, \"running\", \"runner\", and \"ran\" all stem from the root \"run\".\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is it used?\n",
        "1. Simplification: It simplifies textual data, reducing the complexity of subsequent NLP tasks.\n",
        "2. Speed: Stemming is generally faster than more complex methods like lemmatization because it uses simple heuristics.\n",
        "3. Efficiency: It increases the efficiency of information retrieval by linking words with the same roots.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How does it work?\n",
        "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This process is fairly crude; the stems may not be actual words. For example:\n",
        "* Porter Stemmer: One of the most common and gentle stemmers. It's known for its simplicity and speed.\n",
        "* Lancaster Stemmer: A more aggressive stemmer than the Porter, often resulting in shorter stems, hence more errors if accuracy is critical.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges with Stemming:\n",
        "* Over-stemming: Occurs when two words are stemmed to the same root that are not of the same root. This can lead to a loss of information important for understanding the original word.\n",
        "* Under-stemming: Occurs when two words that should be stemmed to the same root are not. This can lead to inconsistent results in search queries and information retrieval.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Example in Python:"
      ],
      "metadata": {
        "id": "RT92ZNGlnxgL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGen9pKnmg6b",
        "outputId": "ac9da48e-5e2e-4556-cb3c-40138076e711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stems: ['run', 'eat', 'fli', 'quickli']\n",
            "Lancaster Stems: ['run', 'eat', 'fly', 'quick']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "words = [\"running\", \"eats\", \"flying\", \"quickly\"]\n",
        "porter_stems = [porter.stem(word) for word in words]\n",
        "lancaster_stems = [lancaster.stem(word) for word in words]\n",
        "\n",
        "print(\"Porter Stems:\", porter_stems)\n",
        "print(\"Lancaster Stems:\", lancaster_stems)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "gFzNdxZKnbrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is it?\n",
        "Lemmatization is the process of reducing a word to its base or root form, known as the lemma. Unlike stemming, which crudely chops off word endings to achieve the root form, lemmatization considers the morphological analysis of the word, ensuring that the reduced form is a valid word according to the language's vocabulary. This process involves understanding the context and part of speech of a word in a sentence, as well as its standard form according to a language's rules.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is it important?\n",
        "1. Reduces Complexity: It decreases the complexity of text data by reducing variations of the same word to a single form, which improves the performance of various NLP tasks.\n",
        "2. Improves Accuracy: Since lemmatization keeps the semantic meaning of the word intact, it's more accurate than stemming for tasks that need a higher level of understanding, such as semantic analysis.\n",
        "3. Facilitates Better Text Analysis: By converting words to their base forms, it becomes easier to perform tasks like textual comparison and pattern recognition.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How Does Lemmatization Work?\n",
        "Lemmatization works by using vocabulary and morphological analysis of words. The goal is to remove only inflectional endings and return the base or dictionary form of a word, which is known as the lemma. For instance, the lemma of \"was\" is \"be,\" and the lemma of \"mice\" is \"mouse.\"\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges in Lemmatization\n",
        "* Language Dependency: Lemmatization rules can be complex and highly language-dependent. For example, in English, verbs and nouns are lemmatized differently.\n",
        "* Resource Intensive: It requires more computational resources than stemming, as it needs a complete dictionary of lemmas and morphological analysis.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Example in Python:"
      ],
      "metadata": {
        "id": "HsPrUIrAn4At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')  # Download the necessary lexicons\n",
        "nltk.download('omw-1.4')  # Download the additional WordNet data\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"ran\", \"run\", \"easily\", \"fairer\", \"was\", \"be\", \"mice\"]\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpjNQVbuoHpO",
        "outputId": "a7519d04-3211-4ce9-e212-deb531af190e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'ran', 'run', 'easily', 'fairer', 'wa', 'be', 'mouse']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "wxvJ-G5xouQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is it?\n",
        "Tokenization is a fundamental step in natural language processing (NLP) where text is divided into smaller units called tokens. These tokens can be words, numbers, or punctuation marks. The process helps in preparing text for deeper processing like parsing, part of speech tagging, and sentiment analysis.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is it important?\n",
        "1. Simplification: It simplifies text analysis by breaking down large pieces of text into manageable units.\n",
        "2. Standardization: Tokens become the standard input for most NLP tasks.\n",
        "3. Efficiency: It increases the efficiency of other NLP processes, as they can operate on simplified and standardized data.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Types of Tokenization\n",
        "1. Word Tokenization: Splits text into words. It's the most common form, useful for tasks like frequency analysis and word-level processing.\n",
        "2. Sentence Tokenization: Breaks text into sentences. This is useful for tasks that require understanding the context or meaning of sentences, like summarization.\n",
        "3. Subword Tokenization: Divides words into smaller meaningful units (subwords or morphemes). This is particularly useful in language modeling and machine translation to handle rare words or morphologically rich languages.\n",
        "4. Character Tokenization: This process divides the text into individual characters. This can be useful for modelling character-level languages.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges\n",
        "* Complexity in Different Languages: Languages with no clear word boundaries (like Chinese or Japanese) require more sophisticated techniques beyond whitespace-based tokenization.\n",
        "* Handling Special Cases: Punctuation, contractions (like \"don't\"), and special characters can complicate straightforward splits."
      ],
      "metadata": {
        "id": "4MpJGtw1o0pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download the necessary models\n",
        "\n",
        "text = \"Hello, how are you doing today?\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF_dGLBdpILi",
        "outputId": "03758fbd-9986-41f2-bb46-f4d56b951a01"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking"
      ],
      "metadata": {
        "id": "mLtpcO3apa47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is it?\n",
        "Chunking, also known as shallow parsing, is the process of extracting phrases from unstructured text and grouping together consecutive words into larger unitsâ€”commonly known as \"chunks.\" Instead of just identifying parts of speech, chunking groups words into meaningful sequences like noun phrases or verb phrases that provide more structure than individual words for processing.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why is it important?\n",
        "1. Structure Extraction: Chunking helps extract more structure from text than individual tokenization or POS tagging by identifying the constituents of sentences.\n",
        "2. Information Retrieval: It aids in extracting entities (like names or places) and relations between them, which is crucial for tasks like information extraction and named entity recognition.\n",
        "3. Improves Understanding: By identifying phrases, chunking helps in understanding the context and the syntactic meaning of the text better.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How Does Chunking Work?\n",
        "Chunking usually works on top of part-of-speech (POS) tagging and uses rules or machine learning models to identify the different chunks. For example, a simple rule might be to group any combination of an optional determiner followed by any number of adjectives and then a noun into a noun phrase (NP).\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Challenges in Chunking\n",
        "* Complex Patterns: Designing rules that accurately capture the intended chunks without being too general or too specific can be challenging.\n",
        "* Language Dependence: Chunking rules can be highly dependent on the language and may not transfer well from one language to another without adjustments."
      ],
      "metadata": {
        "id": "ogFrqF3tpcHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Define your chunk pattern\n",
        "# Rules for Chunking: A common way to perform chunking is to use regular-expression-based rules. For example:\n",
        "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "# This rule states that a noun phrase, NP, might start with an optional determiner (DT), followed by any number of adjectives (JJ), and ends with a noun (NN).\n",
        "\n",
        "# Create a chunk parser\n",
        "cp = RegexpParser(pattern)\n",
        "cs = cp.parse(tagged_tokens)\n",
        "\n",
        "# Display the chunked sentence\n",
        "print(cs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuUXgBrUpyeV",
        "outputId": "498c4ba1-b298-46d3-a0ff-868bd3cfa525"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All-in-One Code"
      ],
      "metadata": {
        "id": "CcfWfJ5grOrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Libraries\n",
        "%%capture\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer"
      ],
      "metadata": {
        "id": "Ds5aPjaHscXo"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentence):\n",
        "  # Tokenization\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "  print('Tokenization: ', tokens)\n",
        "\n",
        "  # Lemmatizer\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "  print('Lemmatization: ', lemmas)\n",
        "\n",
        "  # Stemming\n",
        "  porter = PorterStemmer()\n",
        "  lancaster = LancasterStemmer()\n",
        "\n",
        "  porter_stems = [porter.stem(token) for token in tokens]\n",
        "  lancaster_stems = [lancaster.stem(token) for token in tokens]\n",
        "\n",
        "  print(\"Stemming (Porter Stems): \", porter_stems)\n",
        "  print(\"Stemming (Lancaster Stems): \", lancaster_stems)\n",
        "\n",
        "\n",
        "  # Chunking\n",
        "  tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "  # Define your chunk pattern - A common way to perform chunking is to use regular-expression-based rules. For example:\n",
        "  pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "  # This rule states that a noun phrase, NP, might start with an optional determiner (DT), followed by any number of adjectives (JJ),\n",
        "  # and ends with a noun (NN).\n",
        "\n",
        "  # Create a chunk parser\n",
        "  cp = RegexpParser(pattern)\n",
        "  cs = cp.parse(tagged_tokens)\n",
        "  print(\"Chunking: \", cs)"
      ],
      "metadata": {
        "id": "y7CTrORErN_k"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"The quick brown fox jumps over the lazy dog\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2lBnvjEsFLG",
        "outputId": "5b6bbb3e-299f-48e5-caf9-e3f2f04ac68d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "Lemmatization:  ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog']\n",
            "Stemming (Porter Stems):  ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n",
            "Stemming (Lancaster Stems):  ['the', 'quick', 'brown', 'fox', 'jump', 'ov', 'the', 'lazy', 'dog']\n",
            "Chunking:  (S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"We ran the running marathon together.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBL5s9lwts90",
        "outputId": "0a7e9e2b-d036-4eb8-e3d0-fdbbbfa1e5fd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:  ['We', 'ran', 'the', 'running', 'marathon', 'together', '.']\n",
            "Lemmatization:  ['We', 'ran', 'the', 'running', 'marathon', 'together', '.']\n",
            "Stemming (Porter Stems):  ['we', 'ran', 'the', 'run', 'marathon', 'togeth', '.']\n",
            "Stemming (Lancaster Stems):  ['we', 'ran', 'the', 'run', 'marathon', 'togeth', '.']\n",
            "Chunking:  (S\n",
            "  We/PRP\n",
            "  ran/VBD\n",
            "  the/DT\n",
            "  running/VBG\n",
            "  (NP marathon/NN)\n",
            "  together/RB\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nfkZY5UJtzxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}