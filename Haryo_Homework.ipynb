{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwN4Kuw4eYzapDkoxB6UX9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/Haryo_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TX_-PVuIZkp"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"english\")"
      ],
      "metadata": {
        "id": "2UZE5gX0JbTw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_char(batch):\n",
        "  return [[z for z in x.lower()] for x in batch]\n",
        "\n",
        "def build_str_to_idx_dict(splited_inp):\n",
        "  collected_tokens = []\n",
        "  for sp_inp in splited_inp:\n",
        "    collected_tokens += sp_inp\n",
        "  dict_str_to_idx = {j: i for i,j in enumerate(set(collected_tokens))}\n",
        "  return dict_str_to_idx\n",
        "\n",
        "def translate_to_idx(splited_inp, dict_str_to_idx):\n",
        "  tokenized_inp = []\n",
        "  for inp_instance in splited_inp:\n",
        "    tokenized_inp.append([dict_str_to_idx[x] for x in inp_instance])\n",
        "  return tokenized_inp"
      ],
      "metadata": {
        "id": "AoaBCR2tInub"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = dataset['train'][43:45]\n",
        "batch_text = batch[\"text\"]"
      ],
      "metadata": {
        "id": "XYraU-0mImvq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_str_to_idx = ()\n",
        "dict_idx_to_str = ()"
      ],
      "metadata": {
        "id": "mMvtg1SCIwSR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitted_inp_by_char = split_by_char(batch_text)\n",
        "dict_str_to_idx = build_str_to_idx_dict(splitted_inp_by_char)\n",
        "dict_str_to_idx"
      ],
      "metadata": {
        "id": "Rw_5c2kuJX5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_inp_batch = translate_to_idx(splitted_inp_by_char, dict_str_to_idx)\n",
        "translate_to_idx(split_by_char(['ham']), dict_str_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goJVqflkJ1ZT",
        "outputId": "b844fabe-f395-4334-d203-392788b80ffd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12, 14, 19]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = torch.nn.Embedding(1, 666)\n",
        "embedding_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPG4vjsIKANK",
        "outputId": "0a80c7c3-26f8-402d-a2fa-26a93be9459f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(1, 666)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(char_inp_batch[1]))\n",
        "\n",
        "char_inp_batch[1].append(0)\n",
        "char_inp_batch[1].append(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxav3A_MKE6q",
        "outputId": "bb56f071-15bc-4b53-a353-c201b55503c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(char_inp_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV8gi4a0KNEa",
        "outputId": "e939997a-c120-43fb-d4fe-e20af78dbc2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[32, 19, 14,  0,  2, 10, 22,  0,  8,  3,  6, 14, 18, 18,  3, 20, 25, 24,\n",
              "          3, 33, 35,  3, 30, 12,  2, 33, 22,  3,  8, 35, 31,  3,  2, 10,  3, 17,\n",
              "         11, 33, 12,  3,  2, 10,  3,  7, 14, 19, 22,  3, 33, 12, 22,  9,  3, 18,\n",
              "         22, 29,  3, 24, 25, 17,  3,  2, 10, 33, 35,  3, 10,  2, 10, 33, 12,  3,\n",
              "          6, 14,  0, 27,  4, 12, 14,  0,  3, 33, 14,  1, 22,  8,  3, 33, 12, 22,\n",
              "          3, 18, 35,  8,  8, 23,  3,  8, 19,  2, 33, 12,  3, 33, 12, 22,  3,  5,\n",
              "         18, 35, 30, 10,  3,  8, 14, 15, 22, 28,  3, 14, 10, 35, 33, 12, 22,  0,\n",
              "          3, 33, 35,  4,  7, 12,  3, 16, 22, 10,  3, 29, 14,  9, 28, 32,  3],\n",
              "        [21,  4,  8, 33,  3, 19, 14, 29, 22,  3, 14,  3, 16, 18, 35, 33,  3,  6,\n",
              "         35,  0,  3, 14,  3,  6, 14, 10,  3,  6,  2, 13,  3, 14,  5, 35,  4, 33,\n",
              "          3, 12, 35, 30,  3, 33, 35,  3,  0, 35, 13,  1, 28,  3,  2, 33, 34,  8,\n",
              "          3,  7, 35, 10, 10, 14,  3,  5, 22,  3, 13, 14, 18, 18, 22, 29,  3,  5,\n",
              "         22,  6, 35,  0, 22,  3,  7,  0, 14, 15,  2, 33,  9,  3, 26, 28,  3,  2,\n",
              "         34, 18, 18,  3, 16, 35,  8, 33,  3,  2, 33,  3,  5,  9,  3,  6,  0,  2,\n",
              "         29, 14,  9,  3, 13, 14, 10, 34, 33,  3, 30, 14,  2, 33,  3,  6, 35,  0,\n",
              "          3,  4,  3, 33, 35,  3,  0, 22, 14, 29,  3,  2, 33, 28,  3,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Haryo Homework:\n",
        "  1. Create function to do padding automatically (Pad of 0)\n",
        "  2. Tokenize using 2-gram tokinzer\n",
        "  3. Learn About: Word Embedding VS LSTM / BERT\n",
        "'''"
      ],
      "metadata": {
        "id": "RqiWDxWeKPmq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}