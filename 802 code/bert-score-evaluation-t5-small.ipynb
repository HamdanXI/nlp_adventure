{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyN26cQkDZzk5ICLXhAc6RQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/802%20code/bert-score-evaluation-t5-small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nok_cQ80o4B6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets bert-score tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "from bert_score import score\n",
        "import torch\n",
        "\n",
        "t5_small_paradetox_1Token_tokenizer = AutoTokenizer.from_pretrained(\"HamdanXI/t5-small-paradetox-1Token-split-masked\")\n",
        "t5_small_paradetox_1Token_model = AutoModelForSeq2SeqLM.from_pretrained(\"HamdanXI/t5-small-paradetox-1Token-split-masked\")\n",
        "\n",
        "\n",
        "# BART-base-detox (10,000 epochs with the learning rate of 3e-5)\n",
        "bart_base_detox_tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\")\n",
        "bart_base_detox_model = AutoModelForSeq2SeqLM.from_pretrained(\"s-nlp/bart-base-detox\")\n",
        "\n",
        "\n",
        "paradetox_dataset = load_dataset(\"s-nlp/paradetox\")\n",
        "paradetox_1token_dataset = load_dataset(\"HamdanXI/paradetox-1Token-Split\")"
      ],
      "metadata": {
        "id": "dzGIO-zHo6CV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random_indices = random.sample(range(len(paradetox_dataset['train'])), 671)\n",
        "\n",
        "paradetox_randomSample_dataset = paradetox_dataset['train'].select(random_indices)\n",
        "\n",
        "paradetox_randomSample_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86hJRTefgljh",
        "outputId": "7c7ce024-e3d1-4edd-ea2c-0adc3ec0c214"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['en_toxic_comment', 'en_neutral_comment'],\n",
              "    num_rows: 671\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def max_token_length(input, label, tokenizer):\n",
        "  max_token_length_input = max(len(tokenizer.encode(item)) for item in input)\n",
        "  max_token_length_label = max(len(tokenizer.encode(item)) for item in label)\n",
        "\n",
        "  if max_token_length_input > max_token_length_label:\n",
        "      highest_length = max_token_length_input\n",
        "  else:\n",
        "      highest_length = max_token_length_label\n",
        "\n",
        "  return highest_length"
      ],
      "metadata": {
        "id": "Fbi7BHa0ueDX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def generate_predictions(texts, model, tokenizer, highest_length):\n",
        "    predictions = []\n",
        "    for text in tqdm(texts, desc=\"Generating predictions\"):\n",
        "        inputs = tokenizer(text, padding=True, truncation=True, max_length=highest_length, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs)\n",
        "        predictions.extend([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])\n",
        "    return predictions\n",
        "\n",
        "def bert_score_evaluate(input, label, model, tokenizer, highest_length):\n",
        "  predictions = generate_predictions(input, model, tokenizer, highest_length)\n",
        "\n",
        "  # Compute BERT Score\n",
        "  P, R, F1 = score(predictions, label, lang=\"en\", rescale_with_baseline=True)\n",
        "\n",
        "  # Compute and print the average scores\n",
        "  print(f\"Precision: {P.mean()}, Recall: {R.mean()}, F1 Score: {F1.mean()}\")"
      ],
      "metadata": {
        "id": "Hn48JdZ9ubPm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highest_length_1token_t5_small = max_token_length(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], t5_small_paradetox_1Token_tokenizer)\n",
        "highest_length_1token_bart_base = max_token_length(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_detox_tokenizer)"
      ],
      "metadata": {
        "id": "i0OeTjuKi7_q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_evaluate(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], t5_small_paradetox_1Token_model, t5_small_paradetox_1Token_tokenizer, highest_length_1token_t5_small)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXOFDue8jHbv",
        "outputId": "795f27e0-72f6-4f90-b2fd-7ed7888d42c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   0%|          | 0/811 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Generating predictions: 100%|██████████| 811/811 [02:43<00:00,  4.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.89088374376297, Recall: 0.8717514276504517, F1 Score: 0.8811373710632324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_evaluate(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_detox_model, bart_base_detox_tokenizer, highest_length_1token_bart_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "assTLwCNkNMk",
        "outputId": "757c5847-4b87-4b51-c448-f7d002d46163"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating predictions:   0%|          | 0/811 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Generating predictions: 100%|██████████| 811/811 [05:46<00:00,  2.34it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8873310685157776, Recall: 0.8524273633956909, F1 Score: 0.8696255087852478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highest_length_t5_small = max_token_length(paradetox_randomSample_dataset[\"en_toxic_comment\"], paradetox_randomSample_dataset[\"en_neutral_comment\"], t5_small_paradetox_1Token_tokenizer)\n",
        "highest_length_bart_base = max_token_length(paradetox_randomSample_dataset[\"en_toxic_comment\"], paradetox_randomSample_dataset[\"en_neutral_comment\"], bart_base_detox_tokenizer)"
      ],
      "metadata": {
        "id": "1Zeeh8Q2ok23"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_evaluate(paradetox_randomSample_dataset[\"en_toxic_comment\"], paradetox_randomSample_dataset[\"en_neutral_comment\"], t5_small_paradetox_1Token_model, t5_small_paradetox_1Token_tokenizer, highest_length_t5_small)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTTSLbflopRj",
        "outputId": "0e89767d-963a-49b6-be5d-39678e9db90d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   0%|          | 0/671 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Generating predictions: 100%|██████████| 671/671 [02:36<00:00,  4.30it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6002827882766724, Recall: 0.6506548523902893, F1 Score: 0.6244639754295349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_evaluate(paradetox_randomSample_dataset[\"en_toxic_comment\"], paradetox_randomSample_dataset[\"en_neutral_comment\"], bart_base_detox_model, bart_base_detox_tokenizer, highest_length_bart_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDXI_bg_pCsU",
        "outputId": "88dd4b77-3f2e-4503-cca8-cd86f94a4cd0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating predictions:   0%|          | 0/671 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Generating predictions: 100%|██████████| 671/671 [04:52<00:00,  2.30it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7774385213851929, Recall: 0.7530778646469116, F1 Score: 0.7647563219070435\n"
          ]
        }
      ]
    }
  ]
}