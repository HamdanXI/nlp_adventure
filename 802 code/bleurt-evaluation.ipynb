{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNSJoLBhkOOj5IMPbladYuS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/802%20code/bleurt-evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nok_cQ80o4B6",
        "outputId": "ccf54497-3c2e-44b4-a580-f440ecc05e60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
            "Collecting git+https://github.com/google-research/bleurt.git\n",
            "  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-1ya1huv5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-1ya1huv5\n",
            "  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.11.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (2.12.0)\n",
            "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.1.0)\n",
            "Collecting sentencepiece (from BLEURT==0.0.2)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2023.3.post1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.59.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.3.25)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.7.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.2.2)\n",
            "Building wheels for collected packages: BLEURT\n",
            "  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456767 sha256=3a403b44d8c968038403705a7dd81cdf866311ed3229b97737389bf59b74912f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l6t1yyrj/wheels/64/f4/2c/509a6c31b8ebde891a81029fd94f199b1b92f0e7cfc20d417a\n",
            "Successfully built BLEURT\n",
            "Installing collected packages: sentencepiece, BLEURT\n",
            "Successfully installed BLEURT-0.0.2 sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets tqdm\n",
        "!pip install git+https://github.com/google-research/bleurt.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "from datasets import load_metric\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "bleurt_metric = load_metric('bleurt')\n",
        "\n",
        "t5_small_paradetox_1Token_tokenizer = AutoTokenizer.from_pretrained(\"HamdanXI/t5-small-paradetox-1Token-split-masked\")\n",
        "t5_small_paradetox_1Token_model = AutoModelForSeq2SeqLM.from_pretrained(\"HamdanXI/t5-small-paradetox-1Token-split-masked\")\n",
        "\n",
        "\n",
        "bart_base_paradetox_1Token_tokenizer = AutoTokenizer.from_pretrained(\"HamdanXI/bart-base-paradetox-1Token-split-masked\")\n",
        "bart_base_paradetox_1Token_model = AutoModelForSeq2SeqLM.from_pretrained(\"HamdanXI/bart-base-paradetox-1Token-split-masked\")\n",
        "\n",
        "\n",
        "# BART-base-detox (10,000 epochs with the learning rate of 3e-5)\n",
        "bart_base_detox_tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\")\n",
        "bart_base_detox_model = AutoModelForSeq2SeqLM.from_pretrained(\"s-nlp/bart-base-detox\")\n",
        "\n",
        "bart_base_paradetox_split_tokenizer = AutoTokenizer.from_pretrained(\"HamdanXI/bart-base-paradetox-split\")\n",
        "bart_base_paradetox_split_model = AutoModelForSeq2SeqLM.from_pretrained(\"HamdanXI/bart-base-paradetox-split\")\n",
        "\n",
        "\n",
        "paradetox_dataset = load_dataset(\"HamdanXI/paradetox-split\")\n",
        "paradetox_1token_dataset = load_dataset(\"HamdanXI/paradetox-1Token-Split\")"
      ],
      "metadata": {
        "id": "dzGIO-zHo6CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_token_length(input, label, tokenizer):\n",
        "  max_token_length_input = max(len(tokenizer.encode(item)) for item in input)\n",
        "  max_token_length_label = max(len(tokenizer.encode(item)) for item in label)\n",
        "\n",
        "  if max_token_length_input > max_token_length_label:\n",
        "      highest_length = max_token_length_input\n",
        "  else:\n",
        "      highest_length = max_token_length_label\n",
        "\n",
        "  return highest_length"
      ],
      "metadata": {
        "id": "Fbi7BHa0ueDX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(texts, model, tokenizer, highest_length):\n",
        "    predictions = []\n",
        "    for text in tqdm(texts, desc=\"Generating predictions\"):\n",
        "        inputs = tokenizer(text, padding=True, truncation=True, max_length=highest_length, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs)\n",
        "        predictions.extend([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])\n",
        "    return predictions\n",
        "\n",
        "def bleurt_evaluate(input, label, model, tokenizer, highest_length):\n",
        "    predictions = generate_predictions(input, model, tokenizer, highest_length)\n",
        "\n",
        "    score_results = bleurt_metric.compute(predictions=predictions, references=label)\n",
        "\n",
        "    scores = score_results['scores']\n",
        "\n",
        "    average_score = sum(scores) / len(scores) if scores else 0\n",
        "    print(f\"Average BLEURT Score: {average_score}\")"
      ],
      "metadata": {
        "id": "Hn48JdZ9ubPm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highest_length_bart_base_paradetox_1Token = max_token_length(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_paradetox_1Token_tokenizer)\n",
        "bleurt_evaluate(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_paradetox_1Token_model, bart_base_paradetox_1Token_tokenizer, highest_length_bart_base_paradetox_1Token)"
      ],
      "metadata": {
        "id": "54HarT5IAHHk",
        "outputId": "1f941aa7-08e1-4856-9dbd-9f9edebbf9a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating predictions:   0%|          | 0/811 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Generating predictions: 100%|██████████| 811/811 [04:30<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEURT Score: 0.6903570124238693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highest_length_1token_t5_small = max_token_length(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], t5_small_paradetox_1Token_tokenizer)\n",
        "highest_length_1token_bart_base = max_token_length(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_detox_tokenizer)\n",
        "highest_length_1token_bart_base_split = max_token_length(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_paradetox_split_tokenizer)"
      ],
      "metadata": {
        "id": "i0OeTjuKi7_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleurt_evaluate(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], t5_small_paradetox_1Token_model, t5_small_paradetox_1Token_tokenizer, highest_length_1token_t5_small)"
      ],
      "metadata": {
        "id": "QAzBrwSUpwnu",
        "outputId": "f2f0d2f1-3ee5-4b58-e9a9-0884b9870500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEURT Score: 0.574729475563647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleurt_evaluate(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_detox_model, bart_base_detox_tokenizer, highest_length_1token_bart_base)"
      ],
      "metadata": {
        "id": "assTLwCNkNMk",
        "outputId": "ed1ea69c-580f-4322-f8b0-4cbe257998e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating predictions:   0%|          | 0/811 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Generating predictions: 100%|██████████| 811/811 [05:25<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEURT Score: 0.6970971491275086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleurt_evaluate(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_paradetox_split_model, bart_base_paradetox_split_tokenizer, highest_length_1token_bart_base_split)"
      ],
      "metadata": {
        "id": "k2xBSMhIKAEX",
        "outputId": "4f1a07dc-1dd8-4c6e-ab61-5160aa08fbf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|██████████| 811/811 [05:32<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEURT Score: 0.6858602125707522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highest_length_bart_base_paradetox_1Token = max_token_length(paradetox_dataset['test'][\"en_toxic_comment\"], paradetox_dataset['test'][\"en_neutral_comment\"], bart_base_paradetox_1Token_tokenizer)\n",
        "bleurt_evaluate(paradetox_dataset['test'][\"en_toxic_comment\"], paradetox_dataset['test'][\"en_neutral_comment\"], bart_base_paradetox_1Token_model, bart_base_paradetox_1Token_tokenizer, highest_length_bart_base_paradetox_1Token)"
      ],
      "metadata": {
        "id": "Gk_z8VqQALOk",
        "outputId": "a0776d03-d259-431f-bd91-e4346d18886a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|██████████| 671/671 [04:09<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEURT Score: -0.3179505445151944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highest_length_t5_small = max_token_length(paradetox_dataset['test'][\"en_toxic_comment\"], paradetox_dataset['test'][\"en_neutral_comment\"], t5_small_paradetox_1Token_tokenizer)\n",
        "highest_length_bart_base = max_token_length(paradetox_dataset['test'][\"en_toxic_comment\"], paradetox_dataset['test'][\"en_neutral_comment\"], bart_base_detox_tokenizer)\n",
        "highest_length_bart_base_split = max_token_length(paradetox_dataset['test'][\"en_toxic_comment\"], paradetox_dataset['test'][\"en_neutral_comment\"], bart_base_paradetox_split_tokenizer)"
      ],
      "metadata": {
        "id": "1Zeeh8Q2ok23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleurt_evaluate(paradetox_dataset['test'][\"en_toxic_comment\"], paradetox_dataset['test'][\"en_neutral_comment\"], t5_small_paradetox_1Token_model, t5_small_paradetox_1Token_tokenizer, highest_length_t5_small)"
      ],
      "metadata": {
        "id": "iTTSLbflopRj",
        "outputId": "73a7a285-2697-4925-cfa5-ccef77be653f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|██████████| 671/671 [02:21<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEURT Score: -0.44180790879021103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleurt_evaluate(paradetox_dataset['test'][\"en_toxic_comment\"], paradetox_dataset['test'][\"en_neutral_comment\"], bart_base_detox_model, bart_base_detox_tokenizer, highest_length_bart_base)"
      ],
      "metadata": {
        "id": "ZJURyPF7M3XF",
        "outputId": "e3fc330d-2979-46b9-d936-a654e8231031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|██████████| 671/671 [04:28<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEURT Score: 0.2883815431992361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleurt_evaluate(paradetox_dataset['test'][\"en_toxic_comment\"], paradetox_dataset['test'][\"en_neutral_comment\"], bart_base_paradetox_split_model, bart_base_paradetox_split_tokenizer, highest_length_bart_base_split)"
      ],
      "metadata": {
        "id": "ETP9V1pYKejO",
        "outputId": "df0bebb9-e26c-46b1-a2bc-231c0b604fb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|██████████| 671/671 [05:00<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEURT Score: -0.13517926543418057\n"
          ]
        }
      ]
    }
  ]
}