{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyN26cQkDZzk5ICLXhAc6RQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/802%20code/bert-score-evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nok_cQ80o4B6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets bert-score tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "from bert_score import score\n",
        "import torch\n",
        "\n",
        "t5_small_paradetox_1Token_tokenizer = AutoTokenizer.from_pretrained(\"HamdanXI/t5-small-paradetox-1Token-split-masked\")\n",
        "t5_small_paradetox_1Token_model = AutoModelForSeq2SeqLM.from_pretrained(\"HamdanXI/t5-small-paradetox-1Token-split-masked\")\n",
        "\n",
        "\n",
        "# BART-base-detox (10,000 epochs with the learning rate of 3e-5)\n",
        "bart_base_detox_tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\")\n",
        "bart_base_detox_model = AutoModelForSeq2SeqLM.from_pretrained(\"s-nlp/bart-base-detox\")\n",
        "\n",
        "\n",
        "paradetox_dataset = load_dataset(\"s-nlp/paradetox\")\n",
        "paradetox_1token_dataset = load_dataset(\"HamdanXI/paradetox-1Token-Split\")"
      ],
      "metadata": {
        "id": "dzGIO-zHo6CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random_indices = random.sample(range(len(paradetox_dataset['train'])), 671)\n",
        "\n",
        "paradetox_randomSample_dataset = paradetox_dataset['train'].select(random_indices)\n",
        "\n",
        "paradetox_randomSample_dataset"
      ],
      "metadata": {
        "id": "86hJRTefgljh",
        "outputId": "764c1c1d-36b9-4e6b-f2e7-fef673d98be1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['en_toxic_comment', 'en_neutral_comment'],\n",
              "    num_rows: 671\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def max_token_length(input, label, tokenizer):\n",
        "  max_token_length_input = max(len(tokenizer.encode(item)) for item in input)\n",
        "  max_token_length_label = max(len(tokenizer.encode(item)) for item in label)\n",
        "\n",
        "  if max_token_length_input > max_token_length_label:\n",
        "      highest_length = max_token_length_input\n",
        "  else:\n",
        "      highest_length = max_token_length_label\n",
        "\n",
        "  return highest_length"
      ],
      "metadata": {
        "id": "Fbi7BHa0ueDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def generate_predictions(texts, model, tokenizer, highest_length):\n",
        "    predictions = []\n",
        "    for text in tqdm(texts, desc=\"Generating predictions\"):\n",
        "        inputs = tokenizer(text, padding=True, truncation=True, max_length=highest_length, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs)\n",
        "        predictions.extend([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])\n",
        "    return predictions\n",
        "\n",
        "def bert_score_evaluate(input, label, model, tokenizer, highest_length):\n",
        "  predictions = generate_predictions(input, model, tokenizer, highest_length)\n",
        "\n",
        "  # Compute BERT Score\n",
        "  P, R, F1 = score(predictions, label, lang=\"en\", rescale_with_baseline=True)\n",
        "\n",
        "  # Compute and print the average scores\n",
        "  print(f\"Precision: {P.mean()}, Recall: {R.mean()}, F1 Score: {F1.mean()}\")"
      ],
      "metadata": {
        "id": "Hn48JdZ9ubPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highest_length_1token_t5_small = max_token_length(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], t5_small_paradetox_1Token_tokenizer)\n",
        "highest_length_1token_bart_base = max_token_length(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_detox_tokenizer)"
      ],
      "metadata": {
        "id": "i0OeTjuKi7_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_evaluate(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], t5_small_paradetox_1Token_model, t5_small_paradetox_1Token_tokenizer, highest_length_1token_t5_small)"
      ],
      "metadata": {
        "id": "lXOFDue8jHbv",
        "outputId": "73974473-5fe0-43e0-e811-459380b320e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   0%|          | 0/811 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Generating predictions: 100%|██████████| 811/811 [02:49<00:00,  4.79it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.89088374376297, Recall: 0.8717513680458069, F1 Score: 0.8811372518539429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_evaluate(paradetox_1token_dataset['test'][\"en_toxic_comment\"], paradetox_1token_dataset['test'][\"en_neutral_comment\"], bart_base_detox_model, bart_base_detox_tokenizer, highest_length_1token_bart_base)"
      ],
      "metadata": {
        "id": "assTLwCNkNMk",
        "outputId": "59e3ff79-6aa2-4c6f-f1ad-6b3b02f04b3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating predictions:   0%|          | 0/811 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Generating predictions: 100%|██████████| 811/811 [06:23<00:00,  2.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8873310685157776, Recall: 0.8524273633956909, F1 Score: 0.8696255683898926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highest_length_t5_small = max_token_length(paradetox_randomSample_dataset[\"en_toxic_comment\"], paradetox_randomSample_dataset[\"en_neutral_comment\"], t5_small_paradetox_1Token_tokenizer)\n",
        "highest_length_bart_base = max_token_length(paradetox_randomSample_dataset[\"en_toxic_comment\"], paradetox_randomSample_dataset[\"en_neutral_comment\"], bart_base_detox_tokenizer)"
      ],
      "metadata": {
        "id": "1Zeeh8Q2ok23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_evaluate(paradetox_randomSample_dataset[\"en_toxic_comment\"], paradetox_randomSample_dataset[\"en_neutral_comment\"], t5_small_paradetox_1Token_model, t5_small_paradetox_1Token_tokenizer, highest_length_t5_small)"
      ],
      "metadata": {
        "id": "iTTSLbflopRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_evaluate(paradetox_randomSample_dataset[\"en_toxic_comment\"], paradetox_randomSample_dataset[\"en_neutral_comment\"], bart_base_detox_model, bart_base_detox_tokenizer, highest_length_bart_base)"
      ],
      "metadata": {
        "id": "yDXI_bg_pCsU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}