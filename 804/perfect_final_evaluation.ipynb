{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/804/perfect_final_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Models"
      ],
      "metadata": {
        "id": "VEyKUWPRlMq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rvIwH23h9A81"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model_newton, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"HamdanXI/newton_qa_tinyllama\", # \"unsloth/tinyllama\" for 16bit loading\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "model_caesar, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"HamdanXI/caesar_qa_tinyllama\", # \"unsloth/tinyllama\" for 16bit loading\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "model_beethoven, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"HamdanXI/beethoven_qa_tinyllama\", # \"unsloth/tinyllama\" for 16bit loading\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "model_merged_characters, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"HamdanXI/merged_characters_tinyllama\", # \"unsloth/tinyllama\" for 16bit loading\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Models Outputs"
      ],
      "metadata": {
        "id": "8XRU_DrUlG3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Single Output\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Who are you?\", # instruction\n",
        "        \"Newton\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HIQkpELlSnQ",
        "outputId": "fd74c89e-d20a-4468-8f7f-f4e8a5d9f7a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWho are you?\\n\\n### Input:\\nNewton\\n\\n### Response:\\nI am a man of science, and mathematics. I have devoted my life to the study of the natural world and the laws that govern it. I have made many discoveries and made many contributions to the field of physics. I have also written many books and articles on my work.\\n\\n\\n### Discuss']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Models"
      ],
      "metadata": {
        "id": "8MqWFMGrMzwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Multiple Outputs\n",
        "def generate_responses(character_name, instructions_list):\n",
        "    responses = []\n",
        "    for instruction in instructions_list:\n",
        "      inputs = tokenizer([alpaca_prompt.format(instruction, character_name,\"\",)], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "      if character_name == \"Newton\":\n",
        "        model = model_newton\n",
        "      elif character_name == \"Caesar\":\n",
        "        model = model_caesar\n",
        "      elif character_name == \"Beethoven\":\n",
        "        model = model_beethoven\n",
        "\n",
        "      outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "      decoded_output = tokenizer.batch_decode(outputs)\n",
        "      responses.append(decoded_output)\n",
        "\n",
        "    return responses\n",
        "\n",
        "\n",
        "character_name = \"Newton\"\n",
        "instructions_list = [\"What made you think about gravity when you saw the apple fall?\",\n",
        "                     \"How did you figure out the laws of motion?\",\n",
        "                     \"What was your favorite experiment you ever did?\",\n",
        "                     \"Did you have any friends who liked science as much as you?\",\n",
        "                     \"What was the hardest math problem you ever solved?\",\n",
        "                     \"How did you make a telescope?\",\n",
        "                     \"What did you like to do for fun when you weren't doing science?\",\n",
        "                     \"Did you ever make a mistake in your experiments? What happened?\",\n",
        "                     \"What's your favorite invention that wasn't yours?\",\n",
        "                     \"If you could see the future of science, what would you be most excited to learn about?\"]\n",
        "responses_newton = generate_responses(character_name, instructions_list)\n",
        "\n",
        "character_name = \"Caesar\"\n",
        "instructions_list = [\"What was it like to be a leader in Rome?\",\n",
        "                     \"Did you always want to be a ruler when you were a kid?\",\n",
        "                     \"What was your favorite battle and why?\",\n",
        "                     \"How did you communicate with your army during battles?\",\n",
        "                     \"What did you do for fun in ancient Rome?\",\n",
        "                     \"How did you make decisions as a leader?\",\n",
        "                     \"What's the most interesting place you've ever visited?\",\n",
        "                     \"Did you have any pets?\",\n",
        "                     \"Who was your best friend?\",\n",
        "                     \"If you could go back in time, would you change any of your decisions? Why or why not?\"]\n",
        "responses_caesar = generate_responses(character_name, instructions_list)\n",
        "\n",
        "character_name = \"Beethoven\"\n",
        "instructions_list = [\"How did you keep composing music even when you couldn't hear?\",\n",
        "                     \"What's your favorite piece that you've written?\",\n",
        "                     \"Did you have a favorite instrument to play?\",\n",
        "                     \"Who taught you to play music?\",\n",
        "                     \"What inspires you to write music?\",\n",
        "                     \"Did you ever get nervous before your music was performed?\",\n",
        "                     \"What do you do when you're not writing or playing music?\",\n",
        "                     \"Have you ever made a mistake while performing? What happened?\",\n",
        "                     \"Who is your favorite composer other than yourself?\",\n",
        "                     \"If you could listen to one more piece of music, what would it be?\"]\n",
        "responses_beethoven = generate_responses(character_name, instructions_list)"
      ],
      "metadata": {
        "id": "IgBRx18wmffq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract Response only\n",
        "def response_extractor(responses):\n",
        "  final_responses = []\n",
        "  for response in responses:\n",
        "    string = ''.join([str(item) for item in response])\n",
        "    response_start = string.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
        "    response_text = string[response_start:].strip()\n",
        "\n",
        "    # Check and remove \"\\n\\n\\n### Discuss\" if it's at the end of the response text\n",
        "    discuss_marker = \"\\n\\n\\n### Discuss\"\n",
        "    if response_text.endswith(discuss_marker):\n",
        "      # Remove the discuss marker from the end of the response text\n",
        "      response_text = response_text[:-len(discuss_marker)].strip()\n",
        "\n",
        "    # Store the extracted text\n",
        "    final_responses.append(response_text)\n",
        "\n",
        "  return final_responses"
      ],
      "metadata": {
        "id": "RF2phTdxokQS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_responses_newton = response_extractor(responses_newton)\n",
        "final_responses_caesar = response_extractor(responses_caesar)\n",
        "final_responses_beethoven = response_extractor(responses_beethoven)"
      ],
      "metadata": {
        "id": "GySXt8QVnudv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merged Model"
      ],
      "metadata": {
        "id": "s22vjWKqM3F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Multiple Outputs\n",
        "def generate_responses_merged(character_name, instructions_list):\n",
        "    responses = []\n",
        "    for instruction in instructions_list:\n",
        "      inputs = tokenizer([alpaca_prompt.format(instruction, character_name,\"\",)], return_tensors = \"pt\").to(\"cuda\")\n",
        "      outputs = model_merged_characters.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "      decoded_output = tokenizer.batch_decode(outputs)\n",
        "      responses.append(decoded_output)\n",
        "\n",
        "    return responses\n",
        "\n",
        "\n",
        "character_name = \"Newton\"\n",
        "instructions_list = [\"What made you think about gravity when you saw the apple fall?\",\n",
        "                     \"How did you figure out the laws of motion?\",\n",
        "                     \"What was your favorite experiment you ever did?\",\n",
        "                     \"Did you have any friends who liked science as much as you?\",\n",
        "                     \"What was the hardest math problem you ever solved?\",\n",
        "                     \"How did you make a telescope?\",\n",
        "                     \"What did you like to do for fun when you weren't doing science?\",\n",
        "                     \"Did you ever make a mistake in your experiments? What happened?\",\n",
        "                     \"What's your favorite invention that wasn't yours?\",\n",
        "                     \"If you could see the future of science, what would you be most excited to learn about?\"]\n",
        "responses_newton = generate_responses(character_name, instructions_list)\n",
        "\n",
        "character_name = \"Caesar\"\n",
        "instructions_list = [\"What was it like to be a leader in Rome?\",\n",
        "                     \"Did you always want to be a ruler when you were a kid?\",\n",
        "                     \"What was your favorite battle and why?\",\n",
        "                     \"How did you communicate with your army during battles?\",\n",
        "                     \"What did you do for fun in ancient Rome?\",\n",
        "                     \"How did you make decisions as a leader?\",\n",
        "                     \"What's the most interesting place you've ever visited?\",\n",
        "                     \"Did you have any pets?\",\n",
        "                     \"Who was your best friend?\",\n",
        "                     \"If you could go back in time, would you change any of your decisions? Why or why not?\"]\n",
        "responses_caesar = generate_responses(character_name, instructions_list)\n",
        "\n",
        "character_name = \"Beethoven\"\n",
        "instructions_list = [\"How did you keep composing music even when you couldn't hear?\",\n",
        "                     \"What's your favorite piece that you've written?\",\n",
        "                     \"Did you have a favorite instrument to play?\",\n",
        "                     \"Who taught you to play music?\",\n",
        "                     \"What inspires you to write music?\",\n",
        "                     \"Did you ever get nervous before your music was performed?\",\n",
        "                     \"What do you do when you're not writing or playing music?\",\n",
        "                     \"Have you ever made a mistake while performing? What happened?\",\n",
        "                     \"Who is your favorite composer other than yourself?\",\n",
        "                     \"If you could listen to one more piece of music, what would it be?\"]\n",
        "responses_beethoven = generate_responses(character_name, instructions_list)"
      ],
      "metadata": {
        "id": "rILiwp3KM4R0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_responses_merged_model_newton = response_extractor(responses_newton)\n",
        "final_responses_merged_model_caesar = response_extractor(responses_caesar)\n",
        "final_responses_merged_model_beethoven = response_extractor(responses_beethoven)"
      ],
      "metadata": {
        "id": "jvF4mj5ONByV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Accuracy"
      ],
      "metadata": {
        "id": "s52Ok0SO8jtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wikipedia Retrieving and NER"
      ],
      "metadata": {
        "id": "-p_w1j8plJxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Fetch the Wikipedia page content.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_wikipedia(character_name):\n",
        "    if character_name == \"Newton\":\n",
        "      url = 'https://en.wikipedia.org/wiki/Isaac_Newton'\n",
        "    elif character_name == \"Caesar\":\n",
        "      url = 'https://en.wikipedia.org/wiki/Julius_Caesar'\n",
        "    elif character_name == \"Beethoven\":\n",
        "      url = 'https://en.wikipedia.org/wiki/Ludwig_van_Beethoven'\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Step 2: Extract relevant sections from the Wikipedia content.\n",
        "def extract_relevant_sections(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    text_sections = []\n",
        "\n",
        "    # Extract sections that likely contain relevant entities (e.g., biography, works)\n",
        "    for header in soup.find_all(['h2', 'h3']):\n",
        "        nextNode = header\n",
        "        section_text = \"\"\n",
        "        while True:\n",
        "            nextNode = nextNode.nextSibling\n",
        "            if nextNode is None:\n",
        "                break\n",
        "            if hasattr(nextNode, 'name'):\n",
        "                if nextNode.name == \"h2\":\n",
        "                    break\n",
        "                if nextNode.name in ['p', 'ul', 'li']:\n",
        "                    section_text += nextNode.text\n",
        "        if section_text:\n",
        "            text_sections.append(section_text)\n",
        "    return text_sections\n",
        "\n",
        "url = fetch_wikipedia(\"Newton\")\n",
        "fetch_newton_wikipedia_instructions = extract_relevant_sections(url)\n",
        "\n",
        "url = fetch_wikipedia(\"Caesar\")\n",
        "fetch_caesar_wikipedia_instructions = extract_relevant_sections(url)\n",
        "\n",
        "url = fetch_wikipedia(\"Beethoven\")\n",
        "fetch_newton_wikipedia_instructions = extract_relevant_sections(url)"
      ],
      "metadata": {
        "id": "xSzqXeNfjrLw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_entities_from_responses(responses):\n",
        "    all_entities = []\n",
        "    for response in responses:\n",
        "        doc = nlp(response)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        all_entities.extend(entities)\n",
        "\n",
        "    return all_entities\n",
        "\n",
        "wiki_entities_newton = extract_entities_from_responses(fetch_newton_wikipedia_instructions)\n",
        "wiki_entities_caesar = extract_entities_from_responses(fetch_caesar_wikipedia_instructions)\n",
        "wiki_entities_beethoven = extract_entities_from_responses(fetch_newton_wikipedia_instructions)"
      ],
      "metadata": {
        "id": "v6t5pHcLPY-L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve NER"
      ],
      "metadata": {
        "id": "9Gkc12IAyolV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_entities_from_responses(responses):\n",
        "    all_entities = []\n",
        "    for response in responses:\n",
        "        doc = nlp(response)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        all_entities.extend(entities)\n",
        "\n",
        "    return all_entities\n",
        "\n",
        "model_newton_entities = extract_entities_from_responses(final_responses_newton)\n",
        "model_caesar_entities = extract_entities_from_responses(final_responses_caesar)\n",
        "model_beethoven_entities = extract_entities_from_responses(final_responses_beethoven)\n",
        "\n",
        "model_merged_newton_entities = extract_entities_from_responses(final_responses_merged_model_newton)\n",
        "model_merged_caesar_entities = extract_entities_from_responses(final_responses_merged_model_caesar)\n",
        "model_merged_beethoven_entities = extract_entities_from_responses(final_responses_merged_model_beethoven)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7G8bkLpxXl3",
        "outputId": "0212696c-c36a-415a-940d-02a1dfece839"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_responses_newton"
      ],
      "metadata": {
        "id": "tF75lucMP6ui",
        "outputId": "f1444896-2bf1-49c6-abef-b7cb2b004a0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I thought that the apple was falling because of the force of gravity. I was surprised to see it fall, but I knew that it was due to the force of gravity.</s>',\n",
              " 'I observed the motion of objects and the laws of motion. I observed that objects would move in a straight line if they were not acted upon by any force. I also observed that objects would move in a circular motion if they were acted upon by a force. I then deduced that the laws of motion were the same',\n",
              " \"My favorite experiment was the one that I conducted with the apple. I was able to determine the mass of the apple by observing the apple's weight as it fell from the tree. This experiment taught me a lot about the laws of motion and gravity.</s>\",\n",
              " 'Yes, I did. I had a friend named Robert Hooke, who was also a great scientist. He and I used to spend a lot of time together, and we would often discuss our ideas and theories. I was very interested in his work, and I was always eager to learn more about the world',\n",
              " 'The problem I had to solve was the motion of the planets. I had to figure out how the planets moved in relation to the sun. I had to figure out how the planets moved in relation to each other. I had to figure out how the planets moved in relation to the earth. I had',\n",
              " 'I used a piece of glass and a piece of wood. I put the glass in a wooden box and put the wood on top of it. Then I put the wood on top of the glass and put it in a box. Then I put the box in a box and put it in a box. Then I put',\n",
              " 'I enjoyed reading, playing with toys, and playing games. I also enjoyed going to the theatre and seeing plays.</s>',\n",
              " 'Yes, I did. I made a mistake in my experiments. I was trying to determine the relationship between the force and the acceleration of a body. I was using a pendulley to accelerate a ball, and I was trying to determine the force that would be required to keep the ball from falling. However',\n",
              " \"My favorite invention that wasn't mine was the telescope. It allowed me to see the universe in a way that I had never seen before. It was a game changer for me and for the world.</s>\",\n",
              " 'I would be most excited to learn about the development of calculus and the scientific method of observation.</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in model_merged_beethoven_entities:\n",
        "    print(f\"Entity: {entity[0]}, Type: {entity[1]}\")"
      ],
      "metadata": {
        "id": "gs0tUwHUP1uj",
        "outputId": "18c3b3d3-d425-4dad-cabf-97015cfc662b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Odeath of the Fidelity, Type: WORK_OF_ART\n",
            "Entity: Moonlight Sonata, Type: WORK_OF_ART\n",
            "Entity: one, Type: CARDINAL\n",
            "Entity: hours every day, Type: DATE\n",
            "Entity: One, Type: CARDINAL\n",
            "Entity: third, Type: ORDINAL\n",
            "Entity: Ludwig van Beethoven, Type: PERSON\n",
            "Entity: Symphony, Type: ORG\n",
            "Entity: 125, Type: CARDINAL\n",
            "Entity: Ludwig van Beethoven, Type: PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare both NER, and Return Accuracy"
      ],
      "metadata": {
        "id": "bSY0zTfRyuHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_matching_ratio(model_entities, wiki_entities):\n",
        "    # Initialize a counter for matching entities\n",
        "    matching_entities_count = 0\n",
        "\n",
        "    # Convert wiki_entities to a set for faster lookup\n",
        "    wiki_entities_set = set(wiki_entities)\n",
        "\n",
        "    # Iterate through model entities to check for matches\n",
        "    for entity in model_entities:\n",
        "        if entity in wiki_entities_set:\n",
        "            matching_entities_count += 1\n",
        "\n",
        "    # Calculate the ratio\n",
        "    if len(model_entities) == 0:\n",
        "        return 0  # Avoid division by zero\n",
        "    matching_ratio = matching_entities_count / len(model_entities)\n",
        "\n",
        "    return matching_ratio\n",
        "\n",
        "matching_ratio_newton = calculate_matching_ratio(model_newton_entities, wiki_entities_newton)\n",
        "matching_ratio_caesar = calculate_matching_ratio(model_caesar_entities, wiki_entities_caesar)\n",
        "matching_ratio_beethoven = calculate_matching_ratio(model_beethoven_entities, wiki_entities_beethoven)\n",
        "\n",
        "matching_merged_model_newton_ratio = calculate_matching_ratio(model_merged_newton_entities, wiki_entities_newton)\n",
        "matching_merged_model_caesar_ratio = calculate_matching_ratio(model_merged_caesar_entities, wiki_entities_caesar)\n",
        "matching_merged_model_beethoven_ratio = calculate_matching_ratio(model_merged_beethoven_entities, wiki_entities_beethoven)\n",
        "\n",
        "print(f\"Matching Newton Entities Ratio: {matching_ratio_newton:.2f}\")\n",
        "print(f\"Matching Caesar Entities Ratio: {matching_ratio_caesar:.2f}\")\n",
        "print(f\"Matching Beethoven Entities Ratio: {matching_ratio_beethoven:.2f}\")\n",
        "\n",
        "print(f\"Matching Merged Model Newton Entities Ratio: {matching_merged_model_newton_ratio:.2f}\")\n",
        "print(f\"Matching Merged Model Caesar Entities Ratio: {matching_merged_model_caesar_ratio:.2f}\")\n",
        "print(f\"Matching Merged Model Beethoven Entities Ratio: {matching_merged_model_beethoven_ratio:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CE5-4JhkL9m",
        "outputId": "97e94387-a40f-4e96-a0e8-1e222b677f3d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching Newton Entities Ratio: 0.00\n",
            "Matching Caesar Entities Ratio: 0.60\n",
            "Matching Beethoven Entities Ratio: 0.60\n",
            "Matching Merged Model Newton Entities Ratio: 0.00\n",
            "Matching Merged Model Caesar Entities Ratio: 0.60\n",
            "Matching Merged Model Beethoven Entities Ratio: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Originality"
      ],
      "metadata": {
        "id": "X2VFRp6t8vt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def calculate_average_cosine_similarity(responses):\n",
        "    # Initialize a TF-IDF Vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the responses to a TF-IDF matrix\n",
        "    tfidf_matrix = vectorizer.fit_transform(responses)\n",
        "\n",
        "    # Calculate cosine similarity between all pairs of responses\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    # Calculate the average similarity, excluding self-comparisons\n",
        "    n = similarity_matrix.shape[0]\n",
        "    avg_similarity = (np.sum(similarity_matrix) - n) / (n * (n - 1))\n",
        "\n",
        "    return avg_similarity"
      ],
      "metadata": {
        "id": "tAa13hJy8x1G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_similarity_newton = calculate_average_cosine_similarity(final_responses_newton)\n",
        "average_similarity_caesar = calculate_average_cosine_similarity(final_responses_caesar)\n",
        "average_similarity_beethoven = calculate_average_cosine_similarity(final_responses_beethoven)\n",
        "\n",
        "average_similarity_merged_model_newton = calculate_average_cosine_similarity(final_responses_merged_model_newton)\n",
        "average_similarity_merged_model_caesar = calculate_average_cosine_similarity(final_responses_merged_model_caesar)\n",
        "average_similarity_merged_model_beethoven = calculate_average_cosine_similarity(final_responses_merged_model_beethoven)\n",
        "\n",
        "print(f\"Average Cosine Similarity Newton: {average_similarity_newton:.4f}\")\n",
        "print(f\"Average Cosine Similarity Caesar: {average_similarity_caesar:.4f}\")\n",
        "print(f\"Average Cosine Similarity Beethoven: {average_similarity_beethoven:.4f}\")\n",
        "\n",
        "print(f\"Average Cosine Similarity Merged Model Newton: {average_similarity_merged_model_newton:.4f}\")\n",
        "print(f\"Average Cosine Similarity Merged Model Caesar: {average_similarity_merged_model_caesar:.4f}\")\n",
        "print(f\"Average Cosine Similarity Merged Model Beethoven: {average_similarity_merged_model_beethoven:.4f}\")"
      ],
      "metadata": {
        "id": "TMn_GvtINcuT",
        "outputId": "1e491f6c-6487-40c5-8e99-c049b296b7e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Cosine Similarity Newton: 0.1408\n",
            "Average Cosine Similarity Caesar: 0.1120\n",
            "Average Cosine Similarity Beethoven: 0.1379\n",
            "Average Cosine Similarity Merged Model Newton: 0.1408\n",
            "Average Cosine Similarity Merged Model Caesar: 0.1120\n",
            "Average Cosine Similarity Merged Model Beethoven: 0.1379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2cCPd7PtOG4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}