{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/On47k2gQHymG5MZD71SX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/804/ner_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model"
      ],
      "metadata": {
        "id": "VEyKUWPRlMq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rvIwH23h9A81"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"HamdanXI/newton_qa_tinyllama\", # \"unsloth/tinyllama\" for 16bit loading\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wikipedia Retrieving"
      ],
      "metadata": {
        "id": "-p_w1j8plJxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start by retrieving and processing the content of the Isaac Newton Wikipedia page\n",
        "# for entity identification and expansion. This will involve:\n",
        "# 1. Fetching the content of the Wikipedia page.\n",
        "# 2. Extracting relevant text sections that likely contain key entities.\n",
        "# 3. Identifying entities using a pre-trained NER model.\n",
        "# 4. Expanding the entity list with synonyms and related terms where applicable.\n",
        "\n",
        "# Step 1: Fetch the Wikipedia page content.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_newton_wikipedia():\n",
        "    url = 'https://en.wikipedia.org/wiki/Isaac_Newton'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Step 2: Extract relevant sections from the Wikipedia content.\n",
        "def extract_relevant_sections(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    text_sections = []\n",
        "\n",
        "    # Extract sections that likely contain relevant entities (e.g., biography, works)\n",
        "    for header in soup.find_all(['h2', 'h3']):\n",
        "        nextNode = header\n",
        "        section_text = \"\"\n",
        "        while True:\n",
        "            nextNode = nextNode.nextSibling\n",
        "            if nextNode is None:\n",
        "                break\n",
        "            if hasattr(nextNode, 'name'):\n",
        "                if nextNode.name == \"h2\":\n",
        "                    break\n",
        "                if nextNode.name in ['p', 'ul', 'li']:\n",
        "                    section_text += nextNode.text\n",
        "        if section_text:\n",
        "            text_sections.append(section_text)\n",
        "    return text_sections\n",
        "\n",
        "url = fetch_newton_wikipedia()\n",
        "fetch_newton_wikipedia_instructions = extract_relevant_sections(url)"
      ],
      "metadata": {
        "id": "xSzqXeNfjrLw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Model Outputs"
      ],
      "metadata": {
        "id": "8XRU_DrUlG3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Single Output\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Who are you?\", # instruction\n",
        "        \"Newton\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HIQkpELlSnQ",
        "outputId": "d891eda1-b09a-4d99-dd5a-723b59f54ae9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWho are you?\\n\\n### Input:\\nNewton\\n\\n### Response:\\nI am a man of science, and mathematics. I have devoted my life to the study of the natural world and the laws that govern it. I have made many discoveries and made many contributions to the field of physics. I have also written many books and articles on my work.\\n\\n\\n### Discuss']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Multiple Outputs\n",
        "def generate_responses(character_name, instructions_list):\n",
        "    responses = []\n",
        "    for instruction in instructions_list:\n",
        "      inputs = tokenizer([alpaca_prompt.format(instruction, character_name,\"\",)], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "      outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "      decoded_output = tokenizer.batch_decode(outputs)\n",
        "      responses.append(decoded_output)\n",
        "\n",
        "    return responses\n",
        "\n",
        "\n",
        "# Example usage\n",
        "character_name = \"Newton\"\n",
        "instructions_list = [\"Who are you?\", \"Explain your work on gravity.\"]\n",
        "responses = generate_responses(character_name, instructions_list)\n",
        "\n",
        "# Print the responses\n",
        "for response in responses:\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgBRx18wmffq",
        "outputId": "2bc7df89-1758-47f0-82b5-7d9f182a24b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWho are you?\\n\\n### Input:\\nNewton\\n\\n### Response:\\nI am a man of science, and mathematics. I have devoted my life to the study of the natural world and the laws that govern it. I have made many discoveries and made many contributions to the field of physics. I have also written many books and articles on my work.\\n\\n\\n### Discuss']\n",
            "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain your work on gravity.\\n\\n### Input:\\nNewton\\n\\n### Response:\\nMy work on gravity was to study the force of gravity between two objects. I found that the force of gravity is proportional to the product of the masses of the two objects and inversely proportional to the distance between them. I also discovered that the force of gravity is always attractive, and that it is always rep']\n",
            "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhen will my sister start watching normal movies?\\n\\n### Input:\\nNewton\\n\\n### Response:\\nMy sister will start watching movies when she is ready to watch them. She has been watching movies for a long time, and she has a lot of experience with them. She will start watching movies when she is ready, but she will not start watching them right away. She will watch movies when she']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract Response only\n",
        "def response_extractor(responses):\n",
        "  final_responses = []\n",
        "  for response in responses:\n",
        "    string = ''.join([str(item) for item in response])\n",
        "    response_start = string.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
        "    response_text = string[response_start:].strip()\n",
        "\n",
        "    # Check and remove \"\\n\\n\\n### Discuss\" if it's at the end of the response text\n",
        "    discuss_marker = \"\\n\\n\\n### Discuss\"\n",
        "    if response_text.endswith(discuss_marker):\n",
        "      # Remove the discuss marker from the end of the response text\n",
        "      response_text = response_text[:-len(discuss_marker)].strip()\n",
        "\n",
        "    # Store the extracted text\n",
        "    final_responses.append(response_text)\n",
        "\n",
        "  return final_responses"
      ],
      "metadata": {
        "id": "RF2phTdxokQS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_responses = response_extractor(responses)"
      ],
      "metadata": {
        "id": "GySXt8QVnudv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_responses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1KP6zpysVim",
        "outputId": "961e5d6e-1eb2-4839-f90f-84c2d902d247"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am a man of science, and mathematics. I have devoted my life to the study of the natural world and the laws that govern it. I have made many discoveries and made many contributions to the field of physics. I have also written many books and articles on my work.',\n",
              " 'My work on gravity was to study the force of gravity between two objects. I found that the force of gravity is proportional to the product of the masses of the two objects and inversely proportional to the distance between them. I also discovered that the force of gravity is always attractive, and that it is always rep',\n",
              " 'My sister will start watching movies when she is ready to watch them. She has been watching movies for a long time, and she has a lot of experience with them. She will start watching movies when she is ready, but she will not start watching them right away. She will watch movies when she']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve NER"
      ],
      "metadata": {
        "id": "9Gkc12IAyolV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_entities_from_responses(responses):\n",
        "    all_entities = []\n",
        "    for response in responses:\n",
        "        doc = nlp(response)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        all_entities.extend(entities)\n",
        "\n",
        "    return all_entities\n",
        "\n",
        "model_entities = extract_entities_from_responses(final_responses)\n",
        "\n",
        "# Print out the entities and their types\n",
        "for entity in model_entities:\n",
        "    print(f\"Entity: {entity[0]}, Type: {entity[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7G8bkLpxXl3",
        "outputId": "1e7fd9b6-7d9c-464b-e0a8-063485da3c0f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: between two, Type: CARDINAL\n",
            "Entity: two, Type: CARDINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_entities_from_responses(responses):\n",
        "    all_entities = []\n",
        "    for response in responses:\n",
        "        doc = nlp(response)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        all_entities.extend(entities)\n",
        "\n",
        "    return all_entities\n",
        "\n",
        "wiki_entities = extract_entities_from_responses(fetch_newton_wikipedia_instructions)"
      ],
      "metadata": {
        "id": "KSiKh_29z5vS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare both NER, and Return Accuracy"
      ],
      "metadata": {
        "id": "bSY0zTfRyuHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_matching_ratio(model_entities, wiki_entities):\n",
        "    # Initialize a counter for matching entities\n",
        "    matching_entities_count = 0\n",
        "\n",
        "    # Convert wiki_entities to a set for faster lookup\n",
        "    wiki_entities_set = set(wiki_entities)\n",
        "\n",
        "    # Iterate through model entities to check for matches\n",
        "    for entity in model_entities:\n",
        "        if entity in wiki_entities_set:\n",
        "            matching_entities_count += 1\n",
        "\n",
        "    # Calculate the ratio\n",
        "    if len(model_entities) == 0:\n",
        "        return 0  # Avoid division by zero\n",
        "    matching_ratio = matching_entities_count / len(model_entities)\n",
        "\n",
        "    return matching_ratio\n",
        "\n",
        "matching_ratio = calculate_matching_ratio(model_entities, wiki_entities)\n",
        "print(f\"Matching Entities Ratio: {matching_ratio:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CE5-4JhkL9m",
        "outputId": "75ec3065-29ae-465d-9b5e-c6b7bcb8c842"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching Entities Ratio: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l1LjsquFzULJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}