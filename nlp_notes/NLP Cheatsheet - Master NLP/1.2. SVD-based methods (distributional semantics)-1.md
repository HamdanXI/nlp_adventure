Objective: Finding word embeddings that captures some notion of similarity.

**What is distributional semantics?**
The concept of representing the meaning of a word based on the context in which it usually appears. It is dense and can better capture similarity.

**How do SVD-based methods work?**
Loop over a massive dataset and accumulate word co-occurence counts in a matrix X. Then use SVD where the word vectors are the columns of U.

**What is latent semantic analysis (LSA)?**
LSA is a technique of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. To do so, we build a matrix whose coefficients are word counts per document and use SVD to reduce the number of rows while preserving the similarity structure among columns. To compare documents, we compute the cosine similarity between the column vectors representing them.