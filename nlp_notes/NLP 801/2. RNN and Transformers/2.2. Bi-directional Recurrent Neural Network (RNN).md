This description is in the context of **Bidirectional RNNs** (BiRNNs), a type of recurrent neural network designed to overcome the limitation of traditional RNNs that process sequences only in one direction, typically from start to end.

### Bidirectional RNNs:

**1. Two RNNs for Both Directions**:
In a bidirectional RNN, two separate RNN layers are used:

- **Forward RNN**: Processes the sequence from the start to the end. For instance, for the sentence "The cat sat", it processes the tokens in the order "The", "cat", "sat".

- **Backward RNN**: Processes the sequence from the end to the start, essentially in the reverse order. For the same sentence "The cat sat", it processes the tokens in the order "sat", "cat", "The".

As these RNNs process the sequence, they each generate their own set of hidden states for every token in the sequence. The forward RNN's hidden states capture information from the current token and all the preceding tokens, while the backward RNN's hidden states capture information from the current token and all the subsequent tokens.

**2. Combining the Hidden States**:
To harness the information from both directions for each token, the hidden states produced by the forward and backward RNNs for the same token position are combined. The most common method of combining is **concatenation**:

- **Concatenation**: This simply involves placing the hidden states from both RNNs side by side to form a longer vector. For instance, if the forward RNN produces a hidden state vector of size 100 for a token, and the backward RNN also produces a size 100 vector for the same token, concatenating them would result in a size 200 vector for that token.

Other combining methods can include averaging the values or even summing them, but concatenation remains the most popular as it preserves all the information from both directions without any loss.

### Benefits:
By using a bidirectional approach, the model captures context from both before and after each token, creating a richer representation of the sequence. This often results in better performance in tasks such as sequence labeling, machine translation, and sentiment analysis, among others.

In summary, a Bidirectional RNN uses two RNNs to process sequences from both start to end and end to start. The information from both RNNs is then combined, typically through concatenation, to give a full, bidirectional context for each token in the sequence.