Your inquiry touches upon several crucial aspects of sequence modeling, particularly concerning Recurrent Neural Networks (RNNs) and their limitations in handling long-term dependencies. Let's delve into these topics:

### Problem with Long Dependency and O(N) Complexity:

1. **Long Dependency**: RNNs inherently struggle with long-term dependencies due to the way they process sequences. They handle sequences step-by-step, and at each step, they update their hidden state based on the current input and the previous hidden state. This design makes it challenging for RNNs to maintain or recall information from many steps back, especially as sequences grow longer.

2. **Information Traverses in O(N)**: In an RNN, information about each token propagates through the sequence linearly, one step at a time. So, for information from the first token to impact the final token's processing in a sequence, it must be carried through every intermediate step. This results in an O(N) complexity (where N is the sequence length), and it risks the dilution or loss of the information along the way due to the “vanishing gradient” problem, where gradients become smaller and smaller as they are propagated back through time.

### Translation and Memory Capacity:

Considering your example where the word "keren" (meaning 'cool' in Indonesian) needs to be translated, the model requires context (the rest of the sentence) to perform an accurate translation. If "keren" appears late in a long sentence, a standard RNN might struggle because:

1. **Encoding into Fixed-size Vector**: RNNs typically represent the entire input sequence using a single hidden state vector from the final step for further processing (like translation). This fixed-size vector, encoding information from the whole sequence, might not adequately capture all details, especially from early in the sequence. As sequences get longer and more complex, it becomes increasingly challenging to encode all necessary information into this single vector.

2. **Weight Update Complexity**: When you train neural networks, you adjust their weights based on the loss calculated from the output. For sequence models, especially in tasks like translation, the output's correctness often depends heavily on maintaining accurate information from the entire input sequence. When updating the model's weights during backpropagation, you use the chain rule to propagate gradients back through the network. 

    - In the context of RNNs, this involves multiplying gradients step-by-step through all the timesteps (also known as backpropagation through time or BPTT). If you're calculating the loss based on the translation of "keren," the gradient must travel back to every previous timestep in the sequence. This is computationally intensive and becomes increasingly difficult as sequences grow longer due to the aforementioned "vanishing gradient" problem.

### Solutions and Advanced Approaches:

1. **Advanced RNN Types**: LSTMs (Long Short-Term Memory units) and GRUs (Gated Recurrent Units) are more sophisticated types of RNNs designed to better handle long-term dependencies. They accomplish this through gating mechanisms that regulate information flow, allowing them to retain important information and discard the irrelevant.

2. **Attention Mechanisms**: Attention allows a model to focus on different parts of the input sequence as it generates each token in the output sequence, rather than relying on a single fixed-size context vector. This approach has proven highly effective for tasks like machine translation.

3. **Transformers**: The Transformer architecture, introduced in the paper "Attention Is All You Need," relies on self-attention mechanisms to consider all tokens in the sequence simultaneously, regardless of their positions. This structure allows it to handle long-term dependencies more effectively than RNNs and has led to significant advancements in NLP.

In summary, while traditional RNNs face serious challenges with long-term dependencies and information dilution, various strategies and newer architectures have been developed to address these issues, often providing substantial improvements in performance for tasks like machine translation.