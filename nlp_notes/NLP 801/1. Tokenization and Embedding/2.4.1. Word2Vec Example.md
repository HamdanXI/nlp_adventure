## Word2Vec Example

In the context of Word2Vec, the objective is to find word representations that are good at predicting the surrounding words given a target word (or vice versa). Specifically, for the Skip-Gram model, the objective is P(w_t+j | w_t), where {w_t} is the target word and {w_t+j} is a context word within a window around {w_t}. 

The CBOW (Continuous Bag of Words) variant tries to predict the target word given its context, represented as P(w_t | w_t-j , ..., w_t+j).

I'll provide a basic example of implementing the Skip-Gram model using TensorFlow, which will try to maximize P(w_t+j | w_t):

```python
# Sample data
sentences = ["I love machine learning", "I love deep learning", "Deep learning loves me"]

# Tokenization and vocabulary preparation
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1

# Convert sentences to sequences of integers
sequences = tokenizer.texts_to_sequences(sentences)

# Generate training data using skipgrams
pairs, labels = [], []
for sequence in sequences:
    sg = skipgrams(sequence, vocab_size, window_size=2, negative_samples=0)
    pairs.extend(sg[0])
    labels.extend(sg[1])

word_target, word_context = zip(*pairs)
word_target = tf.keras.utils.to_categorical(word_target, vocab_size)
word_context = tf.keras.utils.to_categorical(word_context, vocab_size)

# Create the Skip-Gram model
embedding_dim = 100

model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=1),
    Flatten(),
    Dense(vocab_size, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit([word_target, word_context], labels, epochs=1000, batch_size=32)
```

This is a very basic implementation of the Skip-Gram model. Note that in practice, optimizations like negative sampling are often used instead of the full softmax for computational efficiency.