## Non-deterministic Decoding: Random Sampling

Most sequence generation methods, like greedy decoding or beam search, are deterministic in nature. This means given the same input and model, they'll produce the same output every time. However, non-deterministic decoding introduces an element of randomness in the generation process, leading to varied outputs for the same input.

Random sampling is one such non-deterministic method. Instead of always picking the most probable next token (like in greedy decoding), tokens are sampled from the distribution of the next-token probabilities provided by the model. This means sometimes less probable tokens might be chosen, leading to more diverse and creative outputs.

**Example**:

Imagine you're using a language model to complete the sentence:

"I want to visit ___."

Given this input, the model predicts the following probabilities for the next word:

- Paris: 0.5
- Mars: 0.01
- Australia: 0.49

With greedy decoding, the model would deterministically choose "Paris" every time because it has the highest probability. However, with random sampling:

1. Most of the time (about 50%), it might still choose "Paris".
2. Occasionally (about 49% of the time), it might choose "Australia".
3. Rarely, but still possibly, it might even choose "Mars" (about 1% of the time).

So, with random sampling, you could get any of the following completions:
- "I want to visit Paris."
- "I want to visit Australia."
- "I want to visit Mars."

To control the randomness, a parameter called **temperature** is often used. A temperature value > 1 makes the sampling more random, increasing the chances of less probable tokens being selected. Conversely, a temperature value < 1 makes the sampling more conservative, making it more likely to pick the most probable tokens, but still with some randomness. A temperature of 1 means sampling from the model's original probabilities.

Random sampling is particularly useful when you want diverse outputs or when you want the model to produce more creative and less generic completions. However, the trade-off is that the generated sequences might sometimes be less coherent or make less sense due to the randomness introduced.