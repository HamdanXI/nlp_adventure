## Beam Search Decoding

Beam search decoding is a heuristic search algorithm used in sequence prediction tasks, aiming to improve the quality of the output by considering multiple sequences simultaneously rather than just one (as with greedy decoding). Instead of selecting the single best token at each step, beam search maintains a set number of the best partial sequences (known as "beams"). At each time step, it expands each of these beams with all possible next tokens, then selects the top sequences based on their probabilities.

The number of sequences or "beams" kept at each step is a parameter often called "beam width" or "beam size."

**Example**:

Imagine you're using a language model to predict the next word in the sentence:

"I love to eat ___."

Given this input, the model predicts the following probabilities for the next word:

1. apples: 0.4
2. pizza: 0.35
3. chocolate: 0.25

If you're using a beam size of 2, the top 2 sequences after this step would be:
1. "I love to eat apples."
2. "I love to eat pizza."

On the next step, you'll expand both of these sequences by considering the next word for each. Suppose after "apples" the top predictions are "and" (0.6) and "with" (0.3), and after "pizza" the top predictions are "and" (0.5) and "with" (0.4). The cumulative probabilities would be:

1. "I love to eat apples and" = 0.4 * 0.6 = 0.24
2. "I love to eat apples with" = 0.4 * 0.3 = 0.12
3. "I love to eat pizza and" = 0.35 * 0.5 = 0.175
4. "I love to eat pizza with" = 0.35 * 0.4 = 0.14

With a beam size of 2, you'll keep:
1. "I love to eat apples and"
2. "I love to eat pizza and"

The process continues, expanding on these sequences until a stopping criterion (like a maximum sequence length or an end token) is met.

Beam search strikes a balance between exploration (considering multiple sequences) and exploitation (favoring high-probability sequences). It's more computationally intensive than greedy decoding but often produces better results, especially for tasks where the quality of the entire sequence matters, like machine translation or caption generation. However, it's worth noting that increasing the beam size doesn't always guarantee better results, as it might also increase the chance of generating generic or repetitive sequences.