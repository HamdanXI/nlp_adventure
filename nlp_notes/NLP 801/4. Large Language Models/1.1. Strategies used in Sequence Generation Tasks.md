All the methods you mentioned—greedy decoding, beam search decoding, random sampling, top-k sampling, nucleus (top-p) sampling, and softmax with temperature—are strategies used in sequence generation tasks, especially in the context of language models. They determine how the next token in a sequence is selected. Here's a summary of their relations and differences:

1. **Objective**:
    - **Greedy Decoding, Beam Search**: These aim to find the most probable sequence. They are deterministic methods, meaning they will produce the same output for the same input given a particular model.
    - **Random Sampling, Top-K Sampling, Nucleus Sampling, Softmax with Temperature**: These introduce randomness to the generation process, producing varied outputs for the same input. They control the trade-off between diversity and model accuracy in different ways.

2. **Determinism vs. Diversity**:
    - **Greedy Decoding**: Always picks the most probable next token. Most deterministic.
    - **Beam Search**: Considers multiple sequences simultaneously but narrows down based on probability. Less deterministic than greedy decoding but more than sampling methods.
    - **Random Sampling**: Introduces randomness by sampling based on the model's probability distribution.
    - **Top-K Sampling**: Randomness is introduced, but only the top K probable tokens are considered.
    - **Nucleus Sampling**: Introduces randomness within a dynamic set of top tokens that cumulatively exceed a probability threshold.
    - **Softmax with Temperature**: Adjusts the model's probability distribution before sampling, controlling the level of randomness.

3. **Use Cases**:
    - **Greedy and Beam Search**: Best when high accuracy and coherence are needed, such as in machine translation.
    - **Sampling Methods**: Useful when diversity or creativity is desired, e.g., story generation, chatbots, or any task where multiple plausible outputs are acceptable.

4. **Computational Overhead**:
    - **Greedy Decoding**: Least computationally intensive as it considers only one token at each step.
    - **Beam Search**: Overhead increases with beam width.
    - **Sampling Methods**: Generally have lower overhead than beam search but can vary depending on implementation details.

5. **Interplay**:
    - While each method can be used on its own, in practice, they can also be combined in various ways to achieve desired characteristics. For instance, one could use beam search with temperature-adjusted softmax probabilities to blend the deterministic nature of beam search with the diversity introduced by temperature.

In essence, the choice among these strategies often depends on the specific requirements of the task at hand, the desired balance between determinism and diversity, and computational considerations.