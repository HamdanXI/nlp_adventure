{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS1PpdmG9ZIwjwyWJHPWpa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/bert-paradetox-with-editOps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "vAIzkb1kxnuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEDX1YuFxcjX"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HamdanXI/paradetox_with_editOps\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sEe4FyPxjLK",
        "outputId": "be33e60c-b70f-468e-933f-35a85f9de2d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['en_toxic_comment', 'en_neutral_comment', 'edit_ops'],\n",
              "        num_rows: 19744\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def find_max_lengths(toxic_comments, neutral_comments, edit_operations):\n",
        "    max_len_toxic = max(len(tokenizer.encode(comment)) for comment in toxic_comments)\n",
        "    max_len_neutral = max(len(tokenizer.encode(comment)) for comment in neutral_comments)\n",
        "    max_len_ops = max(len(tokenizer.encode(\" \".join(ops))) for ops in edit_operations)\n",
        "    return max_len_toxic, max_len_neutral, max_len_ops"
      ],
      "metadata": {
        "id": "5_wN6ROO1EeA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_comments = [item['en_toxic_comment'] for item in dataset['train']]\n",
        "neutral_comment = [item['en_neutral_comment'] for item in dataset['train']]\n",
        "edit_operations = []"
      ],
      "metadata": {
        "id": "DaL4sMZy00FX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dataset['train']:\n",
        "    ops_as_string = ' '.join([' '.join(op) for op in item['edit_ops']])\n",
        "    edit_operations.append(ops_as_string)"
      ],
      "metadata": {
        "id": "C5jzNOid1nJw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_toxic, max_len_neutral, max_len_ops = find_max_lengths(toxic_comments, neutral_comment, edit_operations)\n",
        "\n",
        "print(f\"Maximum length for toxic comments: {max_len_toxic}\")\n",
        "print(f\"Maximum length for neutral comments: {max_len_neutral}\")\n",
        "print(f\"Maximum length for edit operations: {max_len_ops}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH8ZXKsEzi5H",
        "outputId": "27368249-1044-4a1e-b90d-0e38b6426a1c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length for toxic comments: 35\n",
            "Maximum length for neutral comments: 35\n",
            "Maximum length for edit operations: 197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(samples):\n",
        "    processed_comments = []\n",
        "    labels = []\n",
        "\n",
        "    for sample in samples:\n",
        "        toxic_comment = sample['en_toxic_comment']\n",
        "        edit_ops = sample['edit_ops']\n",
        "\n",
        "        words = toxic_comment.split()\n",
        "\n",
        "        for operation in sorted(edit_ops, key=lambda op: int(op[2]), reverse=True):\n",
        "            op_type, text, index = operation[:3]\n",
        "            index = int(index)\n",
        "\n",
        "            if op_type == \"replace\":\n",
        "                words[index:index+len(text.split())] = ['[MASK]']\n",
        "            elif op_type == \"delete\":\n",
        "                del words[index:index+len(text.split())]\n",
        "            elif op_type == \"insert\":\n",
        "                words.insert(index, '[INSERT]')\n",
        "\n",
        "        masked_comment = ' '.join(words)\n",
        "\n",
        "        encoded_comment = tokenizer.encode_plus(\n",
        "            masked_comment,\n",
        "            max_length=35,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        label_ids = tokenizer.encode_plus(\n",
        "            sample['en_neutral_comment'],\n",
        "            max_length=197,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )['input_ids']\n",
        "\n",
        "        processed_comments.append(encoded_comment)\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    return processed_comments, labels"
      ],
      "metadata": {
        "id": "hY63DuM-ynJ4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_comments, labels = preprocess_data(dataset[\"train\"])"
      ],
      "metadata": {
        "id": "6-RATMrh23N-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttZEKWfw3Nad",
        "outputId": "b415dc9c-1503-4ede-fce5-85aa5af0ead8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "input_ids = torch.cat([c['input_ids'] for c in processed_comments], dim=0)\n",
        "attention_mask = torch.cat([c['attention_mask'] for c in processed_comments], dim=0)\n",
        "labels_prepared = torch.cat(labels, dim=0).squeeze()\n",
        "\n",
        "labels_prepared = labels_prepared[:, :input_ids.size(1)]\n",
        "\n",
        "dataset_tensor = TensorDataset(input_ids, attention_mask, labels_prepared)\n",
        "loader = DataLoader(dataset_tensor, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "PPXEvw4M4a5H"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "yr0oyewN3jgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(3):\n",
        "    for batch in loader:\n",
        "        b_input_ids, b_attention_mask, b_labels = batch\n",
        "        b_input_ids = b_input_ids.to(device)\n",
        "        b_attention_mask = b_attention_mask.to(device)\n",
        "        b_labels = b_labels.to(device)\n",
        "\n",
        "        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNZXFSD73xo3",
        "outputId": "cf278ab5-32a9-44c4-dc53-120fbfd5515b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 16.14881706237793\n",
            "Loss: 12.795496940612793\n",
            "Loss: 17.108789443969727\n",
            "Loss: 15.307631492614746\n",
            "Loss: 14.40674114227295\n",
            "Loss: 16.537792205810547\n",
            "Loss: 15.448683738708496\n",
            "Loss: 15.114798545837402\n",
            "Loss: 14.580933570861816\n",
            "Loss: 15.276163101196289\n",
            "Loss: 15.468316078186035\n",
            "Loss: 15.902769088745117\n",
            "Loss: 15.4613037109375\n",
            "Loss: 15.623305320739746\n",
            "Loss: 16.15754508972168\n",
            "Loss: 14.628803253173828\n",
            "Loss: 14.474635124206543\n",
            "Loss: 15.366050720214844\n",
            "Loss: 15.661334991455078\n",
            "Loss: 14.950448036193848\n",
            "Loss: 14.652499198913574\n",
            "Loss: 15.817520141601562\n",
            "Loss: 16.234935760498047\n",
            "Loss: 16.74676513671875\n",
            "Loss: 13.299561500549316\n",
            "Loss: 17.144176483154297\n",
            "Loss: 15.955431938171387\n",
            "Loss: 14.132960319519043\n",
            "Loss: 14.591659545898438\n",
            "Loss: 16.16320037841797\n",
            "Loss: 15.052130699157715\n",
            "Loss: 16.371063232421875\n",
            "Loss: 15.799471855163574\n",
            "Loss: 14.796860694885254\n",
            "Loss: 16.4093074798584\n",
            "Loss: 15.544042587280273\n",
            "Loss: 16.86667251586914\n",
            "Loss: 15.185837745666504\n",
            "Loss: 15.290634155273438\n",
            "Loss: 14.576803207397461\n",
            "Loss: 15.559380531311035\n",
            "Loss: 16.999034881591797\n",
            "Loss: 16.17801284790039\n",
            "Loss: 13.06382942199707\n",
            "Loss: 15.268795013427734\n",
            "Loss: 16.22463035583496\n",
            "Loss: 14.8765869140625\n",
            "Loss: 16.305334091186523\n",
            "Loss: 16.563228607177734\n",
            "Loss: 16.643789291381836\n",
            "Loss: 16.027915954589844\n",
            "Loss: 14.706515312194824\n",
            "Loss: 15.729783058166504\n",
            "Loss: 15.377052307128906\n",
            "Loss: 14.91107177734375\n",
            "Loss: 14.768681526184082\n",
            "Loss: 16.42906951904297\n",
            "Loss: 16.050710678100586\n",
            "Loss: 15.947717666625977\n",
            "Loss: 16.33111572265625\n",
            "Loss: 15.065778732299805\n",
            "Loss: 15.651388168334961\n",
            "Loss: 15.922222137451172\n",
            "Loss: 15.619205474853516\n",
            "Loss: 15.797323226928711\n",
            "Loss: 16.64535903930664\n",
            "Loss: 15.916440963745117\n",
            "Loss: 16.547571182250977\n",
            "Loss: 17.01018524169922\n",
            "Loss: 16.163497924804688\n",
            "Loss: 16.622074127197266\n",
            "Loss: 15.217473030090332\n",
            "Loss: 15.922586441040039\n",
            "Loss: 14.862452507019043\n",
            "Loss: 16.915348052978516\n",
            "Loss: 15.205010414123535\n",
            "Loss: 16.429731369018555\n",
            "Loss: 13.885594367980957\n",
            "Loss: 16.745166778564453\n",
            "Loss: 14.429030418395996\n",
            "Loss: 15.538466453552246\n",
            "Loss: 16.472524642944336\n",
            "Loss: 16.339218139648438\n",
            "Loss: 17.8262996673584\n",
            "Loss: 15.445385932922363\n",
            "Loss: 14.418893814086914\n",
            "Loss: 16.264205932617188\n",
            "Loss: 17.120573043823242\n",
            "Loss: 15.67143726348877\n",
            "Loss: 14.948410987854004\n",
            "Loss: 15.24608325958252\n",
            "Loss: 14.210525512695312\n",
            "Loss: 17.397563934326172\n",
            "Loss: 14.980603218078613\n",
            "Loss: 14.15703296661377\n",
            "Loss: 14.770421028137207\n",
            "Loss: 15.139473915100098\n",
            "Loss: 16.86611557006836\n",
            "Loss: 16.177316665649414\n",
            "Loss: 14.741511344909668\n",
            "Loss: 16.322551727294922\n",
            "Loss: 15.838581085205078\n",
            "Loss: 16.42206382751465\n",
            "Loss: 14.981359481811523\n",
            "Loss: 13.972569465637207\n",
            "Loss: 15.327795028686523\n",
            "Loss: 17.085878372192383\n",
            "Loss: 16.48627281188965\n",
            "Loss: 15.978751182556152\n",
            "Loss: 16.017677307128906\n",
            "Loss: 16.865434646606445\n",
            "Loss: 15.711968421936035\n",
            "Loss: 15.427180290222168\n",
            "Loss: 14.52298355102539\n",
            "Loss: 15.823962211608887\n",
            "Loss: 15.517056465148926\n",
            "Loss: 15.172736167907715\n",
            "Loss: 18.817638397216797\n",
            "Loss: 15.842280387878418\n",
            "Loss: 16.403940200805664\n",
            "Loss: 16.007343292236328\n",
            "Loss: 12.376727104187012\n",
            "Loss: 17.18121337890625\n",
            "Loss: 15.81639575958252\n",
            "Loss: 14.662395477294922\n",
            "Loss: 13.475357055664062\n",
            "Loss: 16.586441040039062\n",
            "Loss: 14.236832618713379\n",
            "Loss: 16.21338653564453\n",
            "Loss: 15.285951614379883\n",
            "Loss: 14.00468921661377\n",
            "Loss: 15.66925048828125\n",
            "Loss: 14.17850399017334\n",
            "Loss: 13.642215728759766\n",
            "Loss: 13.823867797851562\n",
            "Loss: 16.248628616333008\n",
            "Loss: 15.004703521728516\n",
            "Loss: 15.962372779846191\n",
            "Loss: 17.009918212890625\n",
            "Loss: 14.832688331604004\n",
            "Loss: 16.324453353881836\n",
            "Loss: 13.222200393676758\n",
            "Loss: 16.65184783935547\n",
            "Loss: 15.093052864074707\n",
            "Loss: 17.826337814331055\n",
            "Loss: 15.190265655517578\n",
            "Loss: 14.442947387695312\n",
            "Loss: 16.689910888671875\n",
            "Loss: 16.288339614868164\n",
            "Loss: 15.45758056640625\n",
            "Loss: 13.719182014465332\n",
            "Loss: 14.983696937561035\n"
          ]
        }
      ]
    }
  ]
}