{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/Lab_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ede8ffb",
      "metadata": {
        "id": "5ede8ffb"
      },
      "source": [
        "Implement any algorithm from the lecture to solve the grid game (e.g., Q-Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "671a0347",
      "metadata": {
        "id": "671a0347"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dfa1c990",
      "metadata": {
        "id": "dfa1c990"
      },
      "outputs": [],
      "source": [
        "# Environment Class\n",
        "class Grid:\n",
        "    def __init__(self, width, height, start):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.i = start[0]\n",
        "        self.j = start[1]\n",
        "\n",
        "    def set_grid(self, rewards, actions):\n",
        "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "        self.rewards = rewards\n",
        "        self.actions = actions\n",
        "\n",
        "    def set_state(self, s):\n",
        "        self.i = s[0]\n",
        "        self.j = s[1]\n",
        "\n",
        "    def current_state(self):\n",
        "        return (self.i, self.j)\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        return s not in self.actions\n",
        "\n",
        "    def move(self, action):\n",
        "        # check if legal move first\n",
        "        if action in self.actions[(self.i, self.j)]:\n",
        "            if action == 'U':\n",
        "                self.i -= 1\n",
        "            elif action == 'D':\n",
        "                self.i += 1\n",
        "            elif action == 'R':\n",
        "                self.j += 1\n",
        "            elif action == 'L':\n",
        "                self.j -= 1\n",
        "        # return a reward (if any)\n",
        "        return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "    def game_over(self):\n",
        "        # returns true if game is over, else false\n",
        "        # true if we are in a state where no actions are possible\n",
        "        return (self.i, self.j) not in self.actions\n",
        "\n",
        "    def all_states(self):\n",
        "        # returns either a position that has possible next actions\n",
        "        # or a position that yields a reward\n",
        "        return set(self.actions.keys()) | set(self.rewards.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2871951a",
      "metadata": {
        "id": "2871951a"
      },
      "outputs": [],
      "source": [
        "### CREATE GRID OBJECT INSTANCE WITH ACTIONS/REWARDS\n",
        "# Step 1: Initialize the Grid object\n",
        "grid = Grid(4, 4, (0, 0))\n",
        "\n",
        "# Step 2: Define rewards and actions\n",
        "rewards = {(3, 3): 1, (2, 3): -1}  # example rewards at positions (3, 3) and (2, 3)\n",
        "actions = {\n",
        "    (0, 0): ['D', 'R'],\n",
        "    (0, 1): ['L', 'R'],\n",
        "    (0, 2): ['L', 'D', 'R'],\n",
        "    # ... (define actions for other cells)\n",
        "    (3, 2): ['U', 'R'],\n",
        "    (3, 3): []  # no actions possible at terminal state\n",
        "}\n",
        "\n",
        "# Step 3: Set the grid with rewards and actions\n",
        "grid.set_grid(rewards, actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7179c68e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "7179c68e",
        "outputId": "55af868c-ba9a-4ee5-9127-bf3b9f3e8790"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6d15cfe850b5>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# For each state, assign a random small value for each action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-023169563a89>\u001b[0m in \u001b[0;36mall_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# returns either a position that has possible next actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# or a position that yields a reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Grid' object has no attribute 'actions'"
          ]
        }
      ],
      "source": [
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "### INITIALIZE NECCESSARY DATA STRUCTS (e.g., Q table)\n",
        "import random\n",
        "\n",
        "# Define the grid dimensions and start position\n",
        "grid_width = 4\n",
        "grid_height = 4\n",
        "start_position = (0, 0)\n",
        "\n",
        "# Initialize the Grid\n",
        "grid = Grid(grid_width, grid_height, start_position)\n",
        "\n",
        "# Define all possible actions\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "# Initialize the Q-table\n",
        "# For each state, assign a random small value for each action\n",
        "Q = {}\n",
        "states = grid.all_states()\n",
        "for state in states:\n",
        "    Q[state] = {}\n",
        "    for action in ALL_POSSIBLE_ACTIONS:\n",
        "        Q[state][action] = random.uniform(0, 0.1)  # Small random values\n",
        "\n",
        "# Example of how to access Q-value for a specific state-action pair\n",
        "# Q[(1, 2)]['R']  # Q-value for state (1, 2) and action 'R'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e803e225",
      "metadata": {
        "id": "e803e225"
      },
      "outputs": [],
      "source": [
        "### DEFINE HELPER FUNCTIONS, IF NEEDED (e.g., random_action)\n",
        "import random\n",
        "\n",
        "def random_action(a, eps=0.1):\n",
        "    \"\"\"\n",
        "    Choose an action based on epsilon-greedy strategy.\n",
        "    - a: current best action based on max Q-value.\n",
        "    - eps: probability of choosing a random action.\n",
        "    \"\"\"\n",
        "    p = np.random.random()\n",
        "    if p < (1 - eps):\n",
        "        return a\n",
        "    else:\n",
        "        return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "def max_dict(d):\n",
        "    \"\"\"\n",
        "    Returns the key and value with the highest value from dictionary.\n",
        "    \"\"\"\n",
        "    max_key = None\n",
        "    max_val = float('-inf')\n",
        "    for k, v in d.items():\n",
        "        if v > max_val:\n",
        "            max_val = v\n",
        "            max_key = k\n",
        "    return max_key, max_val\n",
        "\n",
        "def print_values(V, g):\n",
        "    \"\"\"\n",
        "    Print the values of the grid.\n",
        "    - V: dictionary of state to value\n",
        "    - g: grid object\n",
        "    \"\"\"\n",
        "    for i in range(g.width):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.height):\n",
        "            v = V.get((i, j), 0)\n",
        "            if v >= 0:\n",
        "                print(\" %.2f|\" % v, end=\"\")\n",
        "            else:\n",
        "                print(\"%.2f|\" % v, end=\"\")  # negative sign takes up an extra space\n",
        "        print(\"\")\n",
        "\n",
        "def print_policy(P, g):\n",
        "    \"\"\"\n",
        "    Print the policy for each grid state.\n",
        "    - P: dictionary of state to action\n",
        "    - g: grid object\n",
        "    \"\"\"\n",
        "    for i in range(g.width):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.height):\n",
        "            a = P.get((i, j), ' ')\n",
        "            print(\"  %s  |\" % a, end=\"\")\n",
        "        print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de1907d",
      "metadata": {
        "id": "8de1907d"
      },
      "outputs": [],
      "source": [
        "### SET ALGORITHM HYPERPARAMETERS\n",
        "### HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c0a68b",
      "metadata": {
        "id": "26c0a68b"
      },
      "outputs": [],
      "source": [
        "s = (0, 0) # start state\n",
        "grid.set_state(s)\n",
        "\n",
        "### IMPLEMENT TRAINING LOOP\n",
        "### HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57eb5037",
      "metadata": {
        "id": "57eb5037"
      },
      "outputs": [],
      "source": [
        "### PRINT FINAL POLICY (actions learnt for each state)\n",
        "### HERE"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}