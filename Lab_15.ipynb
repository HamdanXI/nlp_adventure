{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/Lab_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ede8ffb",
      "metadata": {
        "id": "5ede8ffb"
      },
      "source": [
        "Implement any algorithm from the lecture to solve the grid game (e.g., Q-Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "671a0347",
      "metadata": {
        "id": "671a0347"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dfa1c990",
      "metadata": {
        "id": "dfa1c990"
      },
      "outputs": [],
      "source": [
        "# Environment Class\n",
        "class Grid:\n",
        "    def __init__(self, width, height, start):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.i = start[0]\n",
        "        self.j = start[1]\n",
        "\n",
        "    def set_grid(self, rewards, actions):\n",
        "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "        self.rewards = rewards\n",
        "        self.actions = actions\n",
        "\n",
        "    def set_state(self, s):\n",
        "        self.i = s[0]\n",
        "        self.j = s[1]\n",
        "\n",
        "    def current_state(self):\n",
        "        return (self.i, self.j)\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        return s not in self.actions\n",
        "\n",
        "    def move(self, action):\n",
        "        # check if legal move first\n",
        "        if action in self.actions[(self.i, self.j)]:\n",
        "            if action == 'U':\n",
        "                self.i -= 1\n",
        "            elif action == 'D':\n",
        "                self.i += 1\n",
        "            elif action == 'R':\n",
        "                self.j += 1\n",
        "            elif action == 'L':\n",
        "                self.j -= 1\n",
        "        # return a reward (if any)\n",
        "        return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "    def game_over(self):\n",
        "        # returns true if game is over, else false\n",
        "        # true if we are in a state where no actions are possible\n",
        "        return (self.i, self.j) not in self.actions\n",
        "\n",
        "    def all_states(self):\n",
        "        # returns either a position that has possible next actions\n",
        "        # or a position that yields a reward\n",
        "        return set(self.actions.keys()) | set(self.rewards.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2871951a",
      "metadata": {
        "id": "2871951a"
      },
      "outputs": [],
      "source": [
        "### CREATE GRID OBJECT INSTANCE WITH ACTIONS/REWARDS\n",
        "## HERE\n",
        "\n",
        "# Create a 2x2 grid\n",
        "grid = Grid(2, 2, (0, 0))\n",
        "\n",
        "# Define rewards and actions\n",
        "rewards = {(1, 1): 1, (0, 0): 0, (0, 1): 0, (1, 0): 0}\n",
        "actions = {\n",
        "    (0, 0): ['D', 'R'],\n",
        "    (0, 1): ['L', 'D'],\n",
        "    (1, 0): ['U', 'R'],\n",
        "    # (1, 1) is a terminal state, so no actions are defined\n",
        "}\n",
        "\n",
        "# Set the grid with the defined rewards and actions\n",
        "grid.set_grid(rewards, actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7179c68e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7179c68e",
        "outputId": "fa05854c-bd86-4820-ce24-09f6e5801712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-value for state (0, 0) and action 'U': 0\n"
          ]
        }
      ],
      "source": [
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "### INITIALIZE NECCESSARY DATA STRUCTS (e.g., Q table)\n",
        "### HERE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = {}\n",
        "states = [(i, j) for i in range(2) for j in range(2)]\n",
        "for s in states:\n",
        "    Q[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "        Q[s][a] = 0  # initialize Q-values to 0\n",
        "\n",
        "# Initialize returns, optional\n",
        "returns = {}\n",
        "for s in states:\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "        returns[(s, a)] = []\n",
        "\n",
        "# Initialize state-action count, useful for updating Q-values\n",
        "state_action_count = {}\n",
        "for s in states:\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "        state_action_count[(s, a)] = 0\n",
        "\n",
        "# Example of accessing a Q-value for state (0, 0) and action 'U'\n",
        "print(\"Q-value for state (0, 0) and action 'U':\", Q[(0, 0)]['U'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e803e225",
      "metadata": {
        "id": "e803e225"
      },
      "outputs": [],
      "source": [
        "### DEFINE HELPER FUNCTIONS, IF NEEDED (e.g., random_action)\n",
        "### HERE\n",
        "\n",
        "import random\n",
        "\n",
        "def random_action(a, eps=0.1):\n",
        "    \"\"\"\n",
        "    Choose an action using an epsilon-greedy strategy.\n",
        "    With probability epsilon, we choose a random action.\n",
        "    With probability 1 - epsilon, we choose action 'a'.\n",
        "    \"\"\"\n",
        "    p = np.random.random()\n",
        "    if p < (1 - eps):\n",
        "        return a\n",
        "    else:\n",
        "        return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "def max_dict(d):\n",
        "    \"\"\"\n",
        "    Return the key and value with the highest value from dictionary 'd'.\n",
        "    Here 'd' is expected to be a dictionary of action: Q-value pairs.\n",
        "    \"\"\"\n",
        "    max_key = None\n",
        "    max_val = float('-inf')\n",
        "    for k, v in d.items():\n",
        "        if v > max_val:\n",
        "            max_val = v\n",
        "            max_key = k\n",
        "    return max_key, max_val\n",
        "\n",
        "def update_Q(s, a, r, s_next, alpha, gamma):\n",
        "    \"\"\"\n",
        "    Update Q-value for a state-action pair Q(s,a).\n",
        "    s: current state\n",
        "    a: action taken\n",
        "    r: reward received\n",
        "    s_next: next state\n",
        "    alpha: learning rate\n",
        "    gamma: discount factor\n",
        "    \"\"\"\n",
        "    max_a_next, max_q_next = max_dict(Q[s_next])\n",
        "    Q[s][a] += alpha * (r + gamma * max_q_next - Q[s][a])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8de1907d",
      "metadata": {
        "id": "8de1907d"
      },
      "outputs": [],
      "source": [
        "### SET ALGORITHM HYPERPARAMETERS\n",
        "### HERE\n",
        "\n",
        "# Learning rate\n",
        "alpha = 0.1\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Epsilon for epsilon-greedy strategy\n",
        "epsilon = 0.1\n",
        "\n",
        "# Number of episodes to run the agent\n",
        "num_episodes = 1000\n",
        "\n",
        "# Maximum steps per episode (optional, depending on your environment)\n",
        "max_steps_per_episode = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "26c0a68b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26c0a68b",
        "outputId": "58b0eff6-1a2d-4d71-9ebb-acdb55fc9cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0) {'U': 0.7293753486039924, 'D': 0.5360895095367246, 'L': 0.7851318499205143, 'R': 0.899999999999999}\n",
            "(0, 1) {'U': 0.8757899668957252, 'D': 0.9999999999999996, 'L': 0.7719873547082745, 'R': 0.8417262113984965}\n",
            "(1, 0) {'U': 0.7479857819446121, 'D': 0.05993436755383791, 'L': 0.06568760207834101, 'R': 0.1}\n",
            "(1, 1) {'U': 0, 'D': 0, 'L': 0, 'R': 0}\n"
          ]
        }
      ],
      "source": [
        "s = (0, 0) # start state\n",
        "grid.set_state(s)\n",
        "\n",
        "### IMPLEMENT TRAINING LOOP\n",
        "### HERE\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the state back to the starting state\n",
        "    s = (0, 0)\n",
        "    grid.set_state(s)\n",
        "\n",
        "    for t in range(max_steps_per_episode):\n",
        "        # Choose an action using the epsilon-greedy strategy\n",
        "        a, _ = max_dict(Q[s])\n",
        "        a = random_action(a, epsilon)\n",
        "\n",
        "        # Take the action and observe the new state and reward\n",
        "        r = grid.move(a)\n",
        "        s_next = grid.current_state()\n",
        "\n",
        "        # Update the Q-table\n",
        "        update_Q(s, a, r, s_next, alpha, gamma)\n",
        "\n",
        "        # Update the state\n",
        "        s = s_next\n",
        "\n",
        "        # Check if the state is terminal\n",
        "        if grid.game_over():\n",
        "            break\n",
        "\n",
        "# Optional: Print final Q-values for analysis\n",
        "for s in Q:\n",
        "    print(s, Q[s])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "57eb5037",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57eb5037",
        "outputId": "8c3efdb2-b17d-4c2f-bac4-da97b2925039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Policy:\n",
            "State: (0, 0) -> Action: R\n",
            "State: (0, 1) -> Action: D\n",
            "State: (1, 0) -> Action: U\n",
            "State: (1, 1) -> Action: U\n"
          ]
        }
      ],
      "source": [
        "### PRINT FINAL POLICY (actions learnt for each state)\n",
        "### HERE\n",
        "\n",
        "final_policy = {}\n",
        "for s in Q:\n",
        "    best_action, _ = max_dict(Q[s])\n",
        "    final_policy[s] = best_action\n",
        "\n",
        "# Print the final policy\n",
        "print(\"Final Policy:\")\n",
        "for s in final_policy:\n",
        "    action = final_policy[s]\n",
        "    print(\"State:\", s, \"-> Action:\", action)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}