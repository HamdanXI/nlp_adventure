{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/bert-base-uncased-paradetox-with-labels-with-RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HamdanXI/paradetox_with_labels\")"
      ],
      "metadata": {
        "id": "YLI3ngQTwmhe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "_uGl4ymZyjtp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset\n",
        "from datasets import DatasetDict\n",
        "\n",
        "split = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\n",
        "dataset_split = DatasetDict({\"train\": split[\"train\"], \"test\": split[\"test\"]})"
      ],
      "metadata": {
        "id": "NE7ra4jlyKcR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class TextClassificationEnv(gym.Env):\n",
        "    def __init__(self, dataset):\n",
        "        super(TextClassificationEnv, self).__init__()\n",
        "        self.dataset = dataset[\"train\"]  # We're specifying the split here\n",
        "        self.current_index = 0\n",
        "        self.action_space = spaces.Discrete(2) # toxic or neutral\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(768,)) # Example for BERT's hidden state dimension\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_index = 0\n",
        "        obs = {\"text\": self.dataset[self.current_index]['text']}  # Adjusted to provide the text directly\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        true_label = self.dataset[self.current_index]['label']\n",
        "        reward = 1 if action == true_label else -1\n",
        "        self.current_index += 1\n",
        "        done = self.current_index >= len(self.dataset)\n",
        "        obs = {\"text\": self.dataset[self.current_index]['text']} if not done else None  # Adjusted to provide the text directly\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        pass"
      ],
      "metadata": {
        "id": "88vL7c7HweMY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "class BERTAgent:\n",
        "    def __init__(self, model_path, tokenizer, device=\"cuda\"):\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_path).to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=1e-5)\n",
        "        self.model.train()\n",
        "\n",
        "    def predict(self, state):\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(state['text'], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128).to(self.device)\n",
        "            logits = self.model(**inputs).logits\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            action = torch.multinomial(probs, 1).item()\n",
        "            return action\n",
        "\n",
        "    def optimize(self, state, action, reward):\n",
        "        inputs = self.tokenizer(state['text'], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128).to(self.device)\n",
        "        logits = self.model(**inputs).logits\n",
        "        loss = -F.log_softmax(logits, dim=1)[0, action] * reward  # Policy gradient loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "mRNz3K_VxC5K"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "env = TextClassificationEnv(dataset_split)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_agent = BERTAgent(\"HamdanXI/bert-base-uncased-paradetox_with_labels\", tokenizer)"
      ],
      "metadata": {
        "id": "-P3y3hJhwkXp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = bert_agent.predict(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        bert_agent.optimize(state, action, reward)  # Pass the state as well\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "Na_905PkxRrp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}