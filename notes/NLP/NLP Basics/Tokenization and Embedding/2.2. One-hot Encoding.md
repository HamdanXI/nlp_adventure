---
title: 2.2. One-hot Encoding
updated: 2023-10-12 13:52:33Z
created: 2023-10-12 13:19:22Z
latitude: 25.20484930
longitude: 55.27078280
altitude: 0.0000
---

## One-hot Encoding

One-hot encoding is a way to represent categorical variables as binary vectors. In the context of natural language processing and word embeddings, one-hot encoding involves representing each word in the vocabulary as a unique vector where the position corresponding to that word's index is set to 1, and all other positions are set to 0.

For example, consider a simple vocabulary:
```
Vocabulary = {cat, dog, apple}
```
Using one-hot encoding, the representation would be:
```
cat   -> [1, 0, 0]
dog   -> [0, 1, 0]
apple -> [0, 0, 1]
```

However, when referring to "word embeddings," one-hot encoding is typically not what's meant. Some key points about one-hot encoded vectors:

1. **High Dimensionality**: The size of each one-hot encoded vector is equal to the size of the vocabulary. For a large vocabulary (e.g., 50,000 words), each word will be represented by a vector of length 50,000.

2. **Sparse Representation**: Each vector has only one non-zero entry. This sparsity is memory-inefficient and computationally wasteful for many operations.

3. **No Semantic Information**: One-hot encoded vectors do not capture any semantic information about words. In our example, the vectors for "cat" and "dog" are orthogonal and equally distant from each other as they are from "apple", despite "cat" and "dog" having a closer semantic relationship.

While one-hot encoding is a necessary step in many natural language processing tasks (especially as an input representation for certain models), it is typically not used as the primary method for word embeddings because of the aforementioned limitations.

To obtain dense and meaningful word embeddings that capture semantic relationships, techniques like Word2Vec, GloVe, and FastText are used. These embeddings provide vectors where words with similar meanings have vectors that are closer in the embedding space, and the vectors are much lower in dimensionality than one-hot encoded representations.