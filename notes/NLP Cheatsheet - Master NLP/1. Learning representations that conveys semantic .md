---
title: 1. Learning representations that conveys semantic and syntactic information
updated: 2023-10-12 14:09:44Z
created: 2023-10-12 07:49:39Z
latitude: 25.20484930
longitude: 55.27078280
altitude: 0.0000
---

## 1. Learning representations that conveys semantic and syntactic information
A central problem in NLP is how to represent words as input to any of our models. We need some notions of similarity and distance between words. Intuitively, we do feel that queen is closer to king than cat for instance.

Thus, we want to encode word tokens into some vectors that represent points in some 'word space'.

**Objective**: Finding a N-dimensional space (where N is between 100 and 1000) that is sufficient to encode all semantics of our language. In this word space, each dimension would encode some meaning that we transfer using speech (e.g. tense, count, gender...)

**What are the four main steps of text processing?**
1. **Loading**: Load text as string into memory 
2. **Cleaning**: Cleaning the text, correct misspellings, remove emojies
3. **Tokenization**: Split strings into tokens, where a token could be a word or a character.
4. **Vectorization**: Map all the tokens in data into vectors for ease of feeding into models

**What does bag of words refer to?**
When we put all the words of a document in a 'bucket' we call such a bucket a bag of words. Simply put, a bag of words is a set of all the words in a document.

**What are stop words?**
Stop words are essentially high-frequency generic words that do not convey context-specific sense. E.g.: 'the', 'of', ...

**What is TF-IDF vectorizer?**
TF-IDF stands for term frequency - inverse data frequency. TF is the number of times a word appears in a document divided by the total number of words in the document. IDF is the log of the numbers of documents divided by the number of documents that contain the word w. TF-IDF is the product of those two quantities.

**What is a word embedding?**
Word embedding is a dense representation of words in the form of numeric vectors.