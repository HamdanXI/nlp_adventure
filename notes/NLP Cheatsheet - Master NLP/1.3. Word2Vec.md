---
title: 1.3. Word2Vec
updated: 2023-10-12 08:34:31Z
created: 2023-10-12 07:51:25Z
latitude: 25.20484930
longitude: 55.27078280
altitude: 0.0000
---

**What is an iteration-based model?**
A model that is able to learn one iteration at a time and eventually be able to encode the probability of a word given its context.

**What is Word2Vec?**
A model whose parameters are the word vectors. Train the model on a certain objective. At every iteration, we run our model, evaluate the errors and backpropagate the gradients in the model.

**What are the initial embeddings of Word2Vec model?**
The embedding matrix is initialized randomly using a Normal or uniform distribution. Then, the embedding of word i in the vocabulary is the row i of the embedding matrix.

**What are the two algorithms used by Word2Vec? Explain how they work.**
Continuous bag-of-words (CBOW)
Skip-gram

**What are the two training methods used?**
Hierarchical softmax
Negative sampling

**What is the advantage of Word2Vec over SVD-based methods?**
Much faster to compute and capture complex linguistic patterns beyond word similarity

**What is the limitation of Word2Vec?**
Fails to make use of global co-occurrence statistics. It only relies on local statistics (words in the neighborhood of word i).

E.g.: The cat sat on the mat. Word2Vec doesn't capture if the is a special word in the context of cat or just a stop word.