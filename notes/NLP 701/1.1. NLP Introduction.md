# NLP Introduction

## Levels of Language:

1. **Phonetics/Phonology/Morphology: What words (or subwords) are we dealing with?**
    - **Phonetics**: It is the study of the physical sounds of human speech. This involves examining how sounds are articulated (articulatory phonetics), how they're perceived (auditory phonetics), and their physical properties (acoustic phonetics).
    - **Phonology**: While phonetics concerns the physical sounds of speech, phonology is about the way sounds function within a particular language or languages.
    - **Morphology**: It's the study of the structure of words. Morphemes are the smallest grammatical units in a language that convey meaning. For instance, "unhappiness" has three morphemes: "un-", "happy", and "-ness".

2. **Syntax: What phrases are we dealing with? Which words modify one another?**
    - **Syntax**: It concerns the rules that govern the structure of sentences in a particular language, detailing how words and phrases should be arranged. For example, in English, the typical order is subject-verb-object ("The cat chased the mouse").

3. **Semantics: What’s the literal meaning?**
    - **Semantics**: It delves into the meaning of words, phrases, sentences, and texts. It considers how meaning is constructed and interpreted in language. For example, the word "bark" can mean the sound a dog makes or the outer covering of a tree, depending on the context.

4. **Pragmatics: What should you conclude from the fact that I said something? How should you react?**
    - **Pragmatics**: This examines the ways in which context can influence the interpretation of communication. It's concerned with understanding the speaker's intention and the situation in which a statement is made. For instance, the phrase "Can you pass the salt?" is technically a question, but in context, it's a request.

---

## Why is NLP Difficult?

1. **Computers are not brains**:
    - Humans are believed to have an innate ability to understand and produce language. This idea, rooted in Chomskyan linguistics or Generative Nativism, suggests that humans have an inbuilt grammatical framework. Over time, with theories of language evolution, our brains have evolved to grasp and utilize language in a way that computers simply haven't.

2. **Computers do not socialize**:
    - Communication isn't just about transmitting information; it's deeply social and relies on shared experiences, cultural context, and human emotions. Humans learn language not just to convey facts but to build relationships, express feelings, and understand societal norms. Computers lack this experiential learning.

3. **Key problems**:
    - **Representation of meaning**: Encoding the vast nuances and subtleties of meaning in language into a format that a computer can understand is immensely challenging.
    - **Language presupposes knowledge about the world**: Often, understanding a statement requires broad world knowledge. For instance, knowing that "apples fall from trees" requires an understanding of gravity, apples, and trees.
    - **Language only reflects the surface of meaning (en/decoding)**: Language is replete with idioms, metaphors, and other figures of speech that don't always mean what they literally say. Unpacking this to a 'true' or 'intended' meaning is tricky.
    - **Language presupposes communication between people (inference)**: Much of communication involves reading between the lines or inferring meaning based on shared societal norms or knowledge. This inference is inherently challenging for computers.

In essence, while computers excel in handling vast amounts of data quickly, the multifaceted and deeply human nature of language presents unique challenges for them. Natural Language Processing seeks to bridge this gap, but it's a domain that remains replete with challenges and opportunities.

---

## Types of Ambiguity:

1. **Morphological Ambiguity**:
    - "Joe is a French teacher." Does Joe teach the French language or is Joe a teacher from France?
    - "Joe's exam was hard." Does "Joe's" indicate possession (the exam belonged to Joe) or is it a contraction for "Joe has"?

2. **Phonetic Ambiguity**:
    - "Recognise speech!" vs. "Wreck a nice beach!" These phrases sound similar when spoken aloud but have entirely different meanings.

3. **Part of Speech Ambiguity**:
    - "Joe thinks that milk is horrible." Is "thinks" a verb followed by the conjunction "that", or is "thinks" a noun followed by a relative clause?

4. **Syntactic Ambiguity**:
    - "Call Joe a taxi." Should someone call Joe to inform him about a taxi or should someone order a taxi for Joe?

5. **PP (Prepositional Phrase) Attachment Ambiguity**:
    - "Joe ate pizza with a fork." Did Joe use a fork to eat the pizza, or was the pizza topped with a fork (highly unlikely but syntactically possible)?
    - "Joe ate pizza with meatballs." Were meatballs on the pizza, or did he eat pizza and meatballs separately?
    - "Joe ate pizza with Mike." Was Mike a companion, or was Mike on the pizza?
    - "Joe ate pizza with pleasure." Here, the preposition "with" indicates an emotion or manner.

6. **Sense Ambiguity**:
    - "Joe took the bar exam." Did Joe take an examination for lawyers or some other kind of "bar" related exam?

7. **Modality Ambiguity**:
    - "Joe may win the lottery." Is it expressing the possibility of Joe winning or is it giving permission (however unlikely) for Joe to win?

8. **Scoping Ambiguity**:
    - "Joe likes ripe apples and pears." Does Joe like both ripe apples and ripe pears, or just apples that are ripe and all pears irrespective of their ripeness?

9. **Negation Ambiguity**:
    - "Joe likes his pizza with no cheese and tomatoes." Does Joe want a pizza without both cheese and tomatoes, or does he want a pizza without cheese but with tomatoes?

10. **(Co-)Referential Ambiguity**:
    - "Joe yelled at Mike. He had broken the bike." Who broke the bike—Joe or Mike?
    - "Joe yelled at Mike. He was angry at him." Here, "He" is likely Joe, but there remains a slight ambiguity.

11. **Anaphora Ambiguity**:
    - "John bought him a present." Who is "him"? It could be anyone other than John.
    - "John bought himself a present." Here, "himself" clearly refers to John.

12. **Ellipsis and Parallelism Ambiguity**:
    - "Joe gave Mike a beer and Jeremy a glass of wine." Did Joe give both the beer and the glass of wine, or did Joe give the beer and Jeremy gave the glass of wine?

13. **Quantification Ambiguity**:
    - "Everyone here speaks one language." Does everyone speak the same language or does each person speak only one language (which could be different for each person)?

14. **Metonymy Ambiguity**:
    - "Boston called and left a message for Joe." Clearly, a city can't call, so "Boston" is likely referring to someone from Boston or a representative of Boston. This is an example of metonymy where a related concept is used in place of the actual concept intended.

Each of these examples showcases the vast complexity and nuance present in human language. Ambiguities are one of the main reasons why natural language processing is challenging for computers—they're less adept at navigating the nuances and subtleties that come naturally to humans.

---

## Can't we just memorize it all? 

The question here seems to imply whether language acquisition and understanding can be reduced purely to the act of memorizing words and rules. This is a common notion when people approach language learning; however, language isn't just a list of words and rules. It's a living entity, intertwined with culture, emotion, nuance, and context. 

### The Role of Memorization:

1. **Children's Rapid Vocabulary Acquisition**:
    - **Fast Learning Rate**: Starting from around age two, children undergo a phase of rapid vocabulary acquisition, where they can learn approximately one word every two hours, which amounts to around 9 words a day. 
    - **Single Exposure Learning**: Fascinatingly, children often only need one exposure to a word to associate its meaning. However, this rapid learning isn't flawless. Children can make mistakes in their use of language, exemplified by errors like "I goed to the store," which is an overgeneralization of the past tense rule.
    - **How Do They Do It?**: The exact mechanism behind this accelerated learning remains a topic of scientific study and debate. It's believed to be a mix of innate brain functions, contextual understanding, and pattern recognition.

2. **Adult Vocabulary**:
    - **Typical Vocabulary Size**: The average adult has a passive vocabulary (words they can understand but may not actively use) of about 60,000 words.
    - **Literate Adults**: Adults who are literate, meaning they engage regularly with written content, might have a vocabulary that's about double that of the typical adult. This underscores the role reading plays in vocabulary enhancement.

3. **Animal Word Association**:
    - **Rico the Border Collie**: To emphasize that memorization and word association aren't uniquely human traits, the example of Rico is given. Rico, a border collie from Germany, can associate over 100 toys with their respective names. When asked, he can retrieve a specific toy with an accuracy rate of over 90%. Moreover, his ability to remember the names of new toys after just one encounter is likened to the cognitive capabilities of a three-year-old child.

### Conclusion:
While memorization plays a significant role in language acquisition, it's only one part of a more complex puzzle. Contextual understanding, cultural immersion, exposure to varied linguistic inputs, and practical use all complement memorization in the journey of language mastery. Moreover, the fact that certain animals exhibit word association abilities comparable to young children showcases the universality and depth of cognitive processes across species.


---

## Corpora:

1. **Definition**: 
    - A **corpus** (plural: corpora) is essentially a collection of texts. These texts can serve a multitude of purposes, such as linguistic research, machine learning training, and more.

2. **Features**:
    - **Annotation**: Some corpora are annotated, meaning they have additional information added to them, like part-of-speech tags, syntactic trees, or semantic roles.
    - **Size and Type**: Some corpora consist of massive amounts of plain text, while others are more selective.
    - **Balanced vs. Uniform Corpora**: A balanced corpus represents a variety of genres, topics, or styles, whereas a uniform corpus focuses on one specific type.

3. **Examples**:
    - **Newswire Collections**: These often contain more than 500 million words from news articles.
    - **Brown Corpus**: This contains about 1 million words of "balanced" text, meaning it captures a wide range of genres and topics. The words are also tagged with their parts of speech.
    - **Penn Treebank**: About 1 million words from The Wall Street Journal, with each word parsed into its syntactic components.
    - **Canadian Hansards**: Over 10 million words of sentences that are aligned in both French and English—useful for translation studies.
    - **The Web**: Essentially an immense corpus with trillions of words. Its content is diverse, making it difficult to characterize succinctly (as indicated by the mention of Zipf's law, which describes the frequency distribution of words).

4. **Advantages**:
    - **Access to Live Language**: Corpora often contain contemporary language, showing how language is actually used.
    - **Broad Coverage**: Capture a wide range of linguistic phenomena.
    - **Statistical Information**: Useful for computational linguistics and machine learning.
    - **Validation Tool**: A means to verify linguistic hypotheses or computational models.

5. **Challenges - Corpora Sparseness**:
    - Even with vast amounts of data, corpora can be sparse in certain linguistic phenomena. This sparseness is evident when looking for rare words (unigrams), word pairs (bigrams), or specific syntactic/semantic rules in the data.

---

## The Effective NLP (Natural Language Processing) Cycle:

1. **Pick a Problem**:
    - NLP tasks often involve disambiguating language in some way, such as determining the correct meaning of a word in context.

2. **Gather Data**:
    - For many NLP tasks, a large labeled corpus (a set of texts with relevant annotations) is necessary for training and evaluation.

3. **Initial Model**:
    - Start with a basic model that addresses the problem—often referred to as a baseline.

4. **Iterative Refinement**:
    - **Error Analysis**: After initial testing, review the model's most frequent mistakes.
    - **Human Insight**: Consider how humans might avoid these errors and what cues they might use.
    - **Improve the System**: Incorporate the insights from the above steps to enhance the system. This can involve:
        - **Feature Engineering**: Adjusting or adding input features to the model.
        - **Representation Redesign**: Changing how data is represented to the model.
        - **Algorithm Adjustments**: Trying different machine learning methods or tweaking existing ones.

By cycling through these stages, NLP systems can be incrementally improved, making them more accurate and robust over time.

Certainly! Here's a concise explanation of the topics provided:

---

## A Very Brief History of NLP:

1. **1950s - Early Optimism**:
   - The 1950s marked the beginning of NLP, with early efforts focusing on rule-based methods. There was an initial optimism that machine translation (MT) and language understanding by computers would be straightforward.

2. **1964 - ALPAC Report**:
   - The Automatic Language Processing Advisory Committee (ALPAC) presented a report that expressed skepticism about the progress and promises of machine translation. It recommended a shift towards basic computational linguistics (CL) research.

3. **1990s - Statistical Revolution**:
   - By the '90s, there was a significant shift from rule-based systems to statistical methods. This change was driven by the availability of large text collections (corpora) and the realization that language could be modeled using probabilistic methods based on data.

4. **2010s - Neural Revolution**:
   - The 2010s experienced another paradigm shift with the introduction and success of deep learning in NLP. Pre-training models on vast amounts of data and then fine-tuning them on specific tasks (transfer learning) became a popular and effective approach.

---

## The Neural Frenzy:

1. **Neural/Deep Processing**:
   - **Origin**: Neural network techniques in NLP were inspired by their success in visual analysis tasks.
   - **Complexity Evolution**:
     - From simpler models like perceptrons, the field moved to more complex architectures like recurrent neural networks, convolutional neural networks, LSTMs (Long Short-Term Memory), and other gated models.
     - Each successive architecture brought new capabilities to address different challenges in NLP.
   - **Engineering without Foundations**: The development of neural architectures often felt like a 'blind search', with many models being created without a robust theoretical underpinning.
   - **Remarkable Success**: Despite the experimental nature, neural models achieved incredible success, setting new performance standards in numerous NLP tasks.

2. **Attractiveness and Dangers of Neural Networks**:
   - **Attractive**:
     - Various software packages have made it easier than ever to design and train neural networks, democratizing access to powerful NLP tools.
   - **Dangerous**:
     - The power of neural models can be a double-edged sword. They often require no explicit feature engineering, potentially leading to a reduced emphasis on understanding language's inherent structure and complexities. This can sometimes result in overlooking nuanced linguistic phenomena.

### In Summary:

From the early days of rule-based optimism in the '50s to the deep learning revolution in the 2010s, NLP has undergone significant evolution. While neural models offer state-of-the-art performance, they also present challenges in terms of interpretability and understanding. Balancing the power of these models with a deep understanding of language remains a crucial aspect of advancing the field.

---

## What Can NLP Do Well Today?

1. **Surface-level preprocessing**: This refers to preliminary tasks in NLP.
   - **POS tagging**: Assigning part-of-speech labels to words (e.g., noun, verb, adjective).
   - **Word segmentation**: Splitting a chunk of text into individual words, especially critical for languages that don't use spaces.
   - **NE extraction**: Named Entity extraction, identifying entities like people, organizations, and locations. 
   - **Accuracy**: Typically above 94%.

2. **Shallow syntactic parsing**: 
   - Recognizing syntactic structures without deep analysis.
   - **Accuracy**: Above 92% for many languages, with more in-depth analysis available for some.

3. **IE (Information Extraction)**: 
   - Extracting structured information from unstructured text.
   - **Accuracy**: Around 65% for well-defined topics, benchmarks include MUC (Message Understanding Conference) and ACE (Automatic Content Extraction).

4. **Speech Recognition**: 
   - Transcribing spoken language into text.
   - **Accuracy**: About 80% for a large vocabulary but drops to 20%+ for open vocabulary with noisy input.

5. **IR (Information Retrieval)**: 
   - Retrieving relevant documents or data based on a query.
   - **Accuracy**: Over 40%, benchmarked by TREC (Text Retrieval Conference).

6. **MT (Machine Translation)**: 
   - Translating text from one language to another.
   - **Accuracy**: Around 80%, but this depends on the specific languages and the criteria for measurement.

7. **Summarization**: 
   - Producing a concise version of a longer text.
   - **Accuracy**: Vague but around 70% for extracted summaries, benchmarks include DUC (Document Understanding Conference) and TAC (Text Analysis Conference).

8. **QA (Question Answering)**: 
   - Answering questions based on a given dataset.
   - **Accuracy**: Vague but around 80% for straightforward fact-based questions.

---

## What Cannot NLP Do Today?

1. **General-purpose text generation**: Creating coherent, meaningful, and contextually relevant long texts without specific input remains challenging.

2. **Delivering Semantics**: While NLP can recognize patterns, truly understanding meaning—both theoretically and practically—is still beyond its reach.

3. **Complex Answer Production**: NLP struggles to deliver long or intricate answers by extracting, merging, and summarizing web info.

4. **Extended Dialogues**: Handling prolonged and contextually-rich conversations is still a challenge.

5. **Reading and Learning**: While NLP systems can process and analyze text, they don't "learn" or extend their knowledge in the way humans do.

6. **Using Pragmatics**: Detecting subtleties like style, emotion, and adapting to user profiles are areas where NLP can improve.

7. **Theoretical Contributions**: NLP hasn't significantly contributed to linguistic theories or a comprehensive theory of information in related fields.

8. **Etc.**: The list is not exhaustive, and there are many nuanced linguistic and cognitive tasks that NLP systems aren't proficient at yet.

### In Summary:

NLP has made significant strides in various tasks, from basic preprocessing to complex endeavors like machine translation. However, truly understanding, generating, and interacting with human language in all its depth and nuance remains a frontier. The field is continually evolving, and with advancements in research and technology, some of these challenges may be addressed in the future.