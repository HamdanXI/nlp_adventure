---
title: 2.3. Dot-product
updated: 2023-10-12 13:52:36Z
created: 2023-10-12 13:20:38Z
latitude: 25.20484930
longitude: 55.27078280
altitude: 0.0000
---

## Dot-product

In the context of word embeddings, the dot product is a mathematical operation used to compute the similarity between two vectors, often representing words. It's a way to measure how similar the directions of the two vectors are, regardless of their magnitudes.

Given two vectors {a} and {b}, the dot product is calculated as:

$$
\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i 
$$

Where:
- {n} is the dimensionality of the vectors.
- {a_i} and {b_i} are the components of the vectors {a} and {b}, respectively.

<br>

In the context of word embeddings:
1. **Similarity Measure**: The dot product is often used to measure the similarity between two word vectors. A larger dot product implies that the two vectors are more similar in direction.

2. **Cosine Similarity**: While the dot product gives a measure of similarity, it's affected by the magnitudes of the vectors. Therefore, in many NLP tasks, the cosine similarity, which normalizes by the magnitudes of the vectors, is more commonly used. It's defined as:

$$
\text{cosine similarity}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{||\mathbf{a}||_2 \times ||\mathbf{b}||_2}
$$

Where {ll_a_ll_2} and {ll_b_ll_2} are the L2 norms (Euclidean norms) of vectors {a} and {b} respectively.

3. **Analogy Tasks**: Dot products (or more commonly, cosine similarity) are used in analogy tasks. The classic example is the word analogy "man : king :: woman : ?". To solve this, you might compute vector("king") - vector("man") + vector("woman") and then find the word whose vector has the highest dot product (or cosine similarity) with this resultant vector. The answer often turns out to be "queen".

4. **Information Retrieval**: In information retrieval tasks, the dot product between the query vector and document vectors can help rank documents in terms of relevance to the query.

To summarize, the dot product in word embeddings provides a way to measure the similarity or relatedness between words or between words and phrases, and it underpins many algorithms and tasks in the realm of natural language processing.