---
title: 2.4. Word2Vec
updated: 2023-10-12 13:52:38Z
created: 2023-10-12 13:21:33Z
latitude: 25.20484930
longitude: 55.27078280
altitude: 0.0000
---

## Word2Vec

Word2Vec is a popular technique for learning dense vector representations (embeddings) of words in large corpora. Developed by a team led by Tomas Mikolov at Google in 2013, Word2Vec represents words in continuous vector spaces such that words with similar meanings are closer to each other. This property allows the embeddings to capture semantic relationships between words.

Here are some key points about Word2Vec:

1. **Model Architectures**: Word2Vec comes in two main flavors:
   - **Continuous Bag of Words (CBOW)**: Given a context (a set of surrounding words), CBOW tries to predict the target word that comes in the middle.
   - **Skip-Gram**: Given a target word, Skip-Gram tries to predict its surrounding context words. It's more computationally expensive than CBOW but often produces higher quality embeddings for infrequent words.

2. **Training Process**: Word2Vec uses a shallow neural network (just one hidden layer) for training. However, once trained, the actual neural network is discarded, and the word embeddings (the weights) are retained.

3. **Embedding Properties**: A well-trained Word2Vec model can capture many linguistic patterns. One of the most celebrated properties is its ability to solve word analogies using vector arithmetic. For example, the vector operation "King" - "Man" + "Woman" results in a vector close to "Queen."

4. **Negative Sampling**: Training a neural network on a large vocabulary can be computationally expensive because of the softmax layer's size. To address this, Word2Vec often uses negative sampling, which simplifies the training task to a binary classification problem for a target word and a small number of randomly chosen "negative" words.

5. **Window Size**: Word2Vec uses a sliding window approach to determine context and target word pairs for training. The size of this window (how many words to the left and right of the target word) is a hyperparameter that can be adjusted.

6. **Dimensionality**: Another hyperparameter is the size of the word vectors. Typical values might range from 50 to 300 dimensions, depending on the dataset and application.

7. **Pre-trained Models**: Since Word2Vec requires a lot of data and computational resources to train effectively, researchers often use pre-trained models (e.g., trained on Google News or Wikipedia) and either use these embeddings directly or fine-tune them for a specific task.

8. **Limitations**: Word2Vec has some limitations, notably:
   - It does not consider the order of words (it's not sequence-aware).
   - Each word has a single representation, meaning polysemous words (words with multiple meanings) have the same vector regardless of their context.

Despite its limitations, Word2Vec marked a significant shift in NLP, moving from sparse, high-dimensional representations like one-hot encoding to dense, continuous vector spaces. It's a foundational technique, paving the way for subsequent models and methods in the field.