---
title: 3.1. Subword Tokenization
updated: 2023-10-12 13:55:20Z
created: 2023-10-12 13:54:17Z
latitude: 25.20484930
longitude: 55.27078280
altitude: 0.0000
---

## Subword Tokenization

Subword tokenization is a technique in natural language processing (NLP) where words are divided into smaller units, or subwords. This approach attempts to strike a balance between character-level tokenization and word-level tokenization. Let's delve into the rationale and methods of subword tokenization:

### Why Subword Tokenization?

1. **Handling Rare and Out-of-Vocabulary Words**: Traditional word-level tokenization methods have difficulty with words they haven't seen during training, known as out-of-vocabulary (OOV) words. By breaking words into smaller units, models can potentially handle and generate words they've never seen before.

2. **Morphologically Rich Languages**: Some languages, like Turkish or Finnish, can form long words by stringing together smaller units of meaning. Subword tokenization can capture these morphemes effectively.

3. **Efficiency**: Subword tokenization often leads to a reduced vocabulary size compared to word-level tokenization, especially in multilingual settings, making models more memory-efficient.

### Common Methods:

1. **Byte-Pair Encoding (BPE)**:
   - Initially used for data compression, BPE was adapted for NLP tasks.
   - It starts by tokenizing text at the character level.
   - The method then iteratively merges the most frequent pair of consecutive characters or character sequences until a certain number of merge operations have been performed or a desired vocabulary size is achieved.
   - The resulting subwords (from the merges) can be words themselves or smaller units that can be combined to form words.
   
2. **WordPiece**:
   - WordPiece is similar to BPE but with slight differences in the merging strategy.
   - Instead of always merging the most frequent pairs, WordPiece uses a likelihood maximization approach to determine the best merge operation.

3. **SentencePiece**:
   - SentencePiece is a data-driven unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation tasks.
   - It provides both BPE and a modified version of WordPiece as tokenization algorithms, but it tokenizes raw sentences without any pre-tokenization, making it language agnostic and useful for multilingual models.
   
4. **Unigram Language Model Tokenization**:
   - This method involves training a unigram language model on the tokenized data.
   - The model then iteratively drops tokens (subword units) based on their likelihood until a desired vocabulary size is achieved.

### How It Works:

Let's illustrate with a simplified BPE example:

1. Begin with a word vocabulary (with word frequencies): `{"low": 5, "lower": 2, "newest": 6, "widest": 3}`
2. Tokenize words into character sequences: `{"l o w": 5, "l o w e r": 2, "n e w e s t": 6, "w i d e s t": 3}`
3. Iteratively merge frequent pairs:
   - In the first iteration, "e" and "s" might be the most frequent pair. Merge them: `es`
   - In the next iteration, "es" and "t" might be the most frequent. Merge them: `est`
   - Continue until a stopping criterion (like a set number of merges or vocabulary size) is met.

The result might be subwords like `["l", "o", "w", "e", "r", "n", "w", "es", "t", "i", "d"]`.

### Advantages:

- **Flexibility**: Subword units can represent words, morphemes, or even individual characters, providing flexibility to handle various linguistic phenomena.
- **Generalization**: Enables models to generalize better on unseen data or rare words.
- **Multilingual Models**: Subword tokenization is particularly helpful for multilingual models where a shared subword vocabulary can represent multiple languages.

### Conclusion:

Subword tokenization provides a robust way to represent words, especially in diverse linguistic environments or when working with limited training data. It has become a fundamental component in many state-of-the-art NLP models.