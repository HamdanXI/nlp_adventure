---
title: 2.7. Contextualized Word Embedding
updated: 2023-10-12 13:54:53Z
created: 2023-10-12 08:41:38Z
latitude: 25.20484930
longitude: 55.27078280
altitude: 0.0000
---

## Contextualized Word Embedding

Contextualized word embeddings represent a significant advancement over traditional word embeddings. While traditional embeddings assign a fixed vector to each word regardless of its context, contextualized embeddings provide a dynamic representation based on the word's surrounding context. This means that the same word can have different embeddings based on its usage in different sentences.

Here's a deeper dive into contextualized word embeddings:

**1. Why Context Matters:** The meaning of a word can vary based on its context. Consider the word "bank" in the sentences "I sat by the river bank" and "I deposited money in the bank". Traditional word embeddings would assign the same vector to "bank" in both sentences, while contextualized embeddings would produce different vectors reflective of the distinct usages.

**2. ELMo (Embeddings from Language Models):** One of the first widely recognized models for generating contextualized embeddings. ELMo uses a bidirectional LSTM trained on a language modeling task to produce embeddings. For a given word, its ELMo representation is derived from the internal states of the LSTM, capturing information from both preceding and following words.

**3. BERT (Bidirectional Encoder Representations from Transformers):** Another influential model that produces contextualized embeddings. BERT leverages the Transformer architecture and is pre-trained on a masked language model task. Due to its design, BERT captures deep bidirectional context.

**4. Usage:** Contextualized embeddings have proven beneficial for a range of NLP tasks, from sentiment analysis to named entity recognition and question answering. They can be used by either fine-tuning the entire pre-trained model on a specific task or by extracting embeddings from the pre-trained model and using them as features in another model.

**5. Advantages:**
- Polysemy Handling: Effectively represents words with multiple meanings based on context.
- Richer Representations: Captures nuances in meanings that might be missed by traditional embeddings.
- Transfer Learning: Models like BERT can be fine-tuned on a smaller dataset, leveraging knowledge gained from pre-training on a massive corpus.

<br>

**6. Challenges:**
- Computational Cost: Models like BERT are large and require significant computational resources.
- Complexity: These models can be more challenging to work with, especially for newcomers to NLP.

In summary, contextualized word embeddings have reshaped the NLP landscape by providing more nuanced word representations that take context into account, leading to significant improvements in many tasks.

<br>

### Exam Question

**Given the sentence "I live near the bank of America", how can I get a contextualised word embedding of token 'bank'?**

To get a contextualized word embedding for the token "bank" I would typically use models like ELMo, BERT, or any of their variants and derivatives.

Here's a step-by-step guide using BERT as an example, leveraging the popular Hugging Face's Transformers library:

```
# Load pre-trained model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Tokenize input and obtain output from BERT
text = "I live near the bank of America"
inputs = tokenizer(text, return_tensors="pt", add_special_tokens=True, padding='max_length', max_length=20, truncation=True)
with torch.no_grad():
    output = model(**inputs)

# Extract the embeddings
embeddings = output.last_hidden_state

# Get the embedding for the token "bank"
# Since BERT tokenizes "bank" as a single token, we can find its position in the tokenized sequence
bank_idx = tokenizer.convert_tokens_to_ids("bank")
token_positions = (inputs["input_ids"][0] == bank_idx).nonzero().squeeze().item()
bank_embedding = embeddings[0, token_positions, :].numpy()
```

`bank_embedding` now contains the contextualized embedding for the word "bank" in the provided sentence. This embedding takes into account the context in which "bank" is used in the sentence, making it different from generic embeddings that might be used for the word "bank" in a different context.

Remember that while BERT is a popular choice for contextualized embeddings, other models (like ELMo, RoBERTa, etc.) can also produce contextualized embeddings. The method to obtain the embedding will be similar but may require specific tweaks depending on the model and library you're using.