1.3. Word2Vec

**What is an iteration-based model?**
A model that is able to learn one iteration at a time and eventually be able to encode the probability of a word given its context.

**What is Word2Vec?**
A model whose parameters are the word vectors. Train the model on a certain objective. At every iteration, we run our model, evaluate the errors and backpropagate the gradients in the model.

**What are the initial embeddings of Word2Vec model?**
The embedding matrix is initialized randomly using a Normal or uniform distribution. Then, the embedding of word i in the vocabulary is the row i of the embedding matrix.

**What are the two algorithms used by Word2Vec? Explain how they work.**
Continuous bag-of-words (CBOW)
Skip-gram

**What are the two training methods used?**
Hierarchical softmax
Negative sampling

**What is the advantage of Word2Vec over SVD-based methods?**
Much faster to compute and capture complex linguistic patterns beyond word similarity

**What is the limitation of Word2Vec?**
Fails to make use of global co-occurrence statistics. It only relies on local statistics (words in the neighborhood of word i).

E.g.: The cat sat on the mat. Word2Vec doesn't capture if the is a special word in the context of cat or just a stop word.

id: 4c2cc3d38ea04b04913c20b79f680b37
parent_id: c770b4c8b0e040869d78c13b780409ed
created_time: 2023-10-12T07:51:25.622Z
updated_time: 2023-10-12T08:34:31.016Z
is_conflict: 0
latitude: 25.20484930
longitude: 55.27078280
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2023-10-12T07:51:25.622Z
user_updated_time: 2023-10-12T08:34:31.016Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1