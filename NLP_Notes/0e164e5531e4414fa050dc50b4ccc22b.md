2.4.1. Word2Vec Example

## Word2Vec Example

In the context of Word2Vec, the objective is to find word representations that are good at predicting the surrounding words given a target word (or vice versa). Specifically, for the Skip-Gram model, the objective is P(w_t+j | w_t), where {w_t} is the target word and {w_t+j} is a context word within a window around {w_t}. 

The CBOW (Continuous Bag of Words) variant tries to predict the target word given its context, represented as P(w_t | w_t-j , ..., w_t+j).

I'll provide a basic example of implementing the Skip-Gram model using TensorFlow, which will try to maximize P(w_t+j | w_t):

```python
# Sample data
sentences = ["I love machine learning", "I love deep learning", "Deep learning loves me"]

# Tokenization and vocabulary preparation
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1

# Convert sentences to sequences of integers
sequences = tokenizer.texts_to_sequences(sentences)

# Generate training data using skipgrams
pairs, labels = [], []
for sequence in sequences:
    sg = skipgrams(sequence, vocab_size, window_size=2, negative_samples=0)
    pairs.extend(sg[0])
    labels.extend(sg[1])

word_target, word_context = zip(*pairs)
word_target = tf.keras.utils.to_categorical(word_target, vocab_size)
word_context = tf.keras.utils.to_categorical(word_context, vocab_size)

# Create the Skip-Gram model
embedding_dim = 100

model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=1),
    Flatten(),
    Dense(vocab_size, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit([word_target, word_context], labels, epochs=1000, batch_size=32)
```

This is a very basic implementation of the Skip-Gram model. Note that in practice, optimizations like negative sampling are often used instead of the full softmax for computational efficiency.

id: 0e164e5531e4414fa050dc50b4ccc22b
parent_id: 5b64943bd39842ff837401ac378d9ee9
created_time: 2023-10-12T13:45:41.151Z
updated_time: 2023-10-12T13:52:41.262Z
is_conflict: 0
latitude: 25.20484930
longitude: 55.27078280
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2023-10-12T13:45:41.151Z
user_updated_time: 2023-10-12T13:52:41.262Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1