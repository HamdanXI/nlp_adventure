{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg1MWz7MKHTSS2hy/v/gmQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamdanXI/nlp_adventure/blob/main/Haryo_Homework_1_SOLVED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TX_-PVuIZkp"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"english\")"
      ],
      "metadata": {
        "id": "2UZE5gX0JbTw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_char(batch):\n",
        "  return [[z for z in x.lower()] for x in batch]\n",
        "\n",
        "def split_by_2_words(batch):\n",
        "  return [[z for z in x.lower()] for x in batch]\n",
        "\n",
        "def build_str_to_idx_dict(splited_inp):\n",
        "  collected_tokens = []\n",
        "  for sp_inp in splited_inp:\n",
        "    collected_tokens += sp_inp\n",
        "  dict_str_to_idx = {j: i for i,j in enumerate(set(collected_tokens))}\n",
        "  return dict_str_to_idx\n",
        "\n",
        "def translate_to_idx(splited_inp, dict_str_to_idx):\n",
        "  tokenized_inp = []\n",
        "  for inp_instance in splited_inp:\n",
        "    tokenized_inp.append([dict_str_to_idx[x] for x in inp_instance])\n",
        "  return tokenized_inp"
      ],
      "metadata": {
        "id": "AoaBCR2tInub"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = dataset['train'][43:45]\n",
        "batch_text = batch[\"text\"]"
      ],
      "metadata": {
        "id": "XYraU-0mImvq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_str_to_idx = ()\n",
        "dict_idx_to_str = ()"
      ],
      "metadata": {
        "id": "mMvtg1SCIwSR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitted_inp_by_char = split_by_char(batch_text)\n",
        "dict_str_to_idx = build_str_to_idx_dict(splitted_inp_by_char)\n",
        "dict_str_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw_5c2kuJX5z",
        "outputId": "5920630a-4e62-4693-fefa-308ad438aa14"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'r': 0,\n",
              " 'k': 1,\n",
              " 'i': 2,\n",
              " ' ': 3,\n",
              " 'u': 4,\n",
              " 'b': 5,\n",
              " 'f': 6,\n",
              " 'g': 7,\n",
              " 's': 8,\n",
              " 'y': 9,\n",
              " 'n': 10,\n",
              " '0': 11,\n",
              " 'h': 12,\n",
              " 'c': 13,\n",
              " 'a': 14,\n",
              " 'v': 15,\n",
              " 'p': 16,\n",
              " '1': 17,\n",
              " 'l': 18,\n",
              " 'm': 19,\n",
              " '6': 20,\n",
              " 'j': 21,\n",
              " 'e': 22,\n",
              " ',': 23,\n",
              " '3': 24,\n",
              " '-': 25,\n",
              " '5': 26,\n",
              " 'q': 27,\n",
              " '.': 28,\n",
              " 'd': 29,\n",
              " 'w': 30,\n",
              " 'x': 31,\n",
              " '\"': 32,\n",
              " 't': 33,\n",
              " \"'\": 34,\n",
              " 'o': 35}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_inp_batch = translate_to_idx(splitted_inp_by_char, dict_str_to_idx)\n",
        "translate_to_idx(split_by_char(['ham']), dict_str_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goJVqflkJ1ZT",
        "outputId": "b844fabe-f395-4334-d203-392788b80ffd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12, 14, 19]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = torch.nn.Embedding(1, 666)\n",
        "embedding_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPG4vjsIKANK",
        "outputId": "0a80c7c3-26f8-402d-a2fa-26a93be9459f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(1, 666)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(char_inp_batch[1]))\n",
        "\n",
        "char_inp_batch[1].append(0)\n",
        "char_inp_batch[1].append(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxav3A_MKE6q",
        "outputId": "bb56f071-15bc-4b53-a353-c201b55503c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(char_inp_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV8gi4a0KNEa",
        "outputId": "e939997a-c120-43fb-d4fe-e20af78dbc2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[32, 19, 14,  0,  2, 10, 22,  0,  8,  3,  6, 14, 18, 18,  3, 20, 25, 24,\n",
              "          3, 33, 35,  3, 30, 12,  2, 33, 22,  3,  8, 35, 31,  3,  2, 10,  3, 17,\n",
              "         11, 33, 12,  3,  2, 10,  3,  7, 14, 19, 22,  3, 33, 12, 22,  9,  3, 18,\n",
              "         22, 29,  3, 24, 25, 17,  3,  2, 10, 33, 35,  3, 10,  2, 10, 33, 12,  3,\n",
              "          6, 14,  0, 27,  4, 12, 14,  0,  3, 33, 14,  1, 22,  8,  3, 33, 12, 22,\n",
              "          3, 18, 35,  8,  8, 23,  3,  8, 19,  2, 33, 12,  3, 33, 12, 22,  3,  5,\n",
              "         18, 35, 30, 10,  3,  8, 14, 15, 22, 28,  3, 14, 10, 35, 33, 12, 22,  0,\n",
              "          3, 33, 35,  4,  7, 12,  3, 16, 22, 10,  3, 29, 14,  9, 28, 32,  3],\n",
              "        [21,  4,  8, 33,  3, 19, 14, 29, 22,  3, 14,  3, 16, 18, 35, 33,  3,  6,\n",
              "         35,  0,  3, 14,  3,  6, 14, 10,  3,  6,  2, 13,  3, 14,  5, 35,  4, 33,\n",
              "          3, 12, 35, 30,  3, 33, 35,  3,  0, 35, 13,  1, 28,  3,  2, 33, 34,  8,\n",
              "          3,  7, 35, 10, 10, 14,  3,  5, 22,  3, 13, 14, 18, 18, 22, 29,  3,  5,\n",
              "         22,  6, 35,  0, 22,  3,  7,  0, 14, 15,  2, 33,  9,  3, 26, 28,  3,  2,\n",
              "         34, 18, 18,  3, 16, 35,  8, 33,  3,  2, 33,  3,  5,  9,  3,  6,  0,  2,\n",
              "         29, 14,  9,  3, 13, 14, 10, 34, 33,  3, 30, 14,  2, 33,  3,  6, 35,  0,\n",
              "          3,  4,  3, 33, 35,  3,  0, 22, 14, 29,  3,  2, 33, 28,  3,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Haryo Homework:\n",
        "  1. Create function to do padding automatically (Pad of 0)\n",
        "  2. Tokenize using 2-gram tokenizer\n",
        "  3. Learn About: Word Embedding VS LSTM / BERT\n",
        "'''"
      ],
      "metadata": {
        "id": "RqiWDxWeKPmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_padding_based_on_max_length(batch_text):\n",
        "    # Convert each list of characters into a string\n",
        "    batch_strings = [''.join(text_list) for text_list in batch_text]\n",
        "\n",
        "    # Find the length of the longest string\n",
        "    max_length = max(len(s) for s in batch_strings)\n",
        "\n",
        "    # Pad each string with zeros until they all have the same length\n",
        "    batch_text_padded = [s.ljust(max_length, '0') for s in batch_strings]\n",
        "\n",
        "    return batch_text_padded"
      ],
      "metadata": {
        "id": "h6ToKHnHNieL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(splitted_inp_by_char[1]))\n",
        "splitted_inp_by_char_padded = zero_padding_based_on_max_length(splitted_inp_by_char)\n",
        "print(len(splitted_inp_by_char_padded[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFXA6jrwLLiZ",
        "outputId": "511b002f-762e-487d-c8a1-8c8156edf400"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141\n",
            "143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def two_word_tokens(text):\n",
        "    words = text.split()  # Split the string into individual words\n",
        "    tokens = []\n",
        "\n",
        "    # Loop through the words and create tokens of two words together\n",
        "    for i in range(len(words) - 1):  # Subtracting 1 to avoid index out of range\n",
        "        tokens.append(words[i] + ' ' + words[i+1])\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "DVjhg5TcMgkS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_text_2_gram_tokenizer = [two_word_tokens(s) for s in batch_text]"
      ],
      "metadata": {
        "id": "vTxQBymZddga"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_text_2_gram_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNtknl9Adr2H",
        "outputId": "791f48a9-7e40-4000-aad2-aa173e164779"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['\"Mariners fall',\n",
              "  'fall 6-3',\n",
              "  '6-3 to',\n",
              "  'to White',\n",
              "  'White Sox',\n",
              "  'Sox in',\n",
              "  'in 10th',\n",
              "  '10th in',\n",
              "  'in game',\n",
              "  'game they',\n",
              "  'they led',\n",
              "  'led 3-1',\n",
              "  '3-1 into',\n",
              "  'into ninth',\n",
              "  'ninth Farquhar',\n",
              "  'Farquhar takes',\n",
              "  'takes the',\n",
              "  'the loss,',\n",
              "  'loss, Smith',\n",
              "  'Smith the',\n",
              "  'the blown',\n",
              "  'blown save.',\n",
              "  'save. Another',\n",
              "  'Another tough',\n",
              "  'tough pen',\n",
              "  'pen day.\"'],\n",
              " ['Just made',\n",
              "  'made a',\n",
              "  'a plot',\n",
              "  'plot for',\n",
              "  'for a',\n",
              "  'a fan',\n",
              "  'fan fic',\n",
              "  'fic about',\n",
              "  'about How',\n",
              "  'How To',\n",
              "  'To Rock.',\n",
              "  \"Rock. It's\",\n",
              "  \"It's gonna\",\n",
              "  'gonna be',\n",
              "  'be called',\n",
              "  'called before',\n",
              "  'before gravity',\n",
              "  'gravity 5.',\n",
              "  \"5. I'll\",\n",
              "  \"I'll post\",\n",
              "  'post it',\n",
              "  'it by',\n",
              "  'by Friday',\n",
              "  \"Friday can't\",\n",
              "  \"can't wait\",\n",
              "  'wait for',\n",
              "  'for u',\n",
              "  'u to',\n",
              "  'to read',\n",
              "  'read it.']]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}